# AUTOGENERATED! DO NOT EDIT! File to edit: ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb.

# %% auto 0
__all__ = ['console', 'key', 'tfd', 'uniform', 'truncnormal', 'normal', 'diagnormal', 'mixture_of_diagnormals',
           'mixture_of_normals', 'mixture_of_truncnormals', 'min', 'max', 'normal_logpdf', 'truncnorm_logpdf',
           'truncnorm_pdf', 'constrained_lh_jit', 'b3d_image_likelihood', 'dslice', 'pad', 'mix_std',
           'make_point_cloud_baseline_model', 'get_1d_mixture_components', 'make_constrained_baseline_model',
           'make_truncated_constrained_baseline_model', 'constrained_lh_mix_ij', 'constrained_lh', 'B3DImageLikelihood']

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 4
import jax
from jax import (jit, vmap)
import jax.numpy as jnp

import genjax
from genjax import gen, choice_map, vector_choice_map

import matplotlib.pyplot as plt
import numpy as np

import bayes3d
from bayes3d._mkl.utils import *

console = genjax.pretty(show_locals=False)
key     = jax.random.PRNGKey(0)

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 5
import genjax._src.generative_functions.distributions.tensorflow_probability as gentfp
import tensorflow_probability.substrates.jax as tfp
tfd = tfp.distributions

uniform = genjax.tfp_uniform

truncnormal = gentfp.TFPDistribution(
    lambda mu, sig, low, high: tfd.TruncatedNormal(mu, sig, low, high));

normal = gentfp.TFPDistribution(
    lambda mu, sig: tfd.Normal(mu, sig));

diagnormal = gentfp.TFPDistribution(
    lambda mus, sigs: tfd.MultivariateNormalDiag(mus, sigs));


mixture_of_diagnormals = gentfp.TFPDistribution(
    lambda logps, mus, sig: tfd.MixtureSameFamily(
        tfd.Categorical(logps),
        tfd.MultivariateNormalDiag(mus, sig * jnp.ones_like(mus))))


mixture_of_normals = gentfp.TFPDistribution(
    lambda ws, mus, sigs: tfd.MixtureSameFamily(
        tfd.Categorical(ws),
        tfd.Normal(mus, sigs)))


mixture_of_truncnormals = gentfp.TFPDistribution(
    lambda ws, mus, sigs, lows, highs: tfd.MixtureSameFamily(
        tfd.Categorical(ws),
        tfd.TruncatedNormal(mus, sigs, lows, highs)))

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 7
# Some helper to keep code concise
min = jnp.minimum
max = jnp.maximum


def dslice(X, i, j, w):     
    m = 2*w + 1
    return  jax.lax.dynamic_slice(X, (i, j, 0), (m, m, 3))   


def pad(X, w, val=-100.0):
    return jax.lax.pad(X,  val, ((w,w,0),(w,w,0),(0,0,0)))


def mix_std(ps, mus, stds):
    """Standard Deviation of a mixture of Gaussians."""
    return jnp.sqrt(jnp.sum(ps*stds**2) + jnp.sum(ps*mus**2) - (jnp.sum(ps*mus))**2)

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 9
def make_point_cloud_baseline_model(w, zmax):
    """Returns a uncostrained sensor model with a gaussian outlier model."""

    pad_val = -100.0

    @genjax.drop_arguments
    @gen
    def _sensor_model_ij(i, j, Y_, sig, outlier):
        
        ys = dslice(Y_, i, j, w).reshape(-1,3)

        ws = jnp.zeros(len(ys))
        # TODO: Does zero'ing out the padded values, create some
        # unwanted sideeffect?
        ws = jnp.where(ys[...,2] == pad_val, -jnp.inf, ws)
        ws = ws - logsumexp(ws)
        

        inlier_outlier_mix = genjax.tfp_mixture(genjax.tfp_categorical, 
            [mixture_of_diagnormals, diagnormal])
        
        x = inlier_outlier_mix([jnp.log(1.0-outlier), jnp.log(outlier)], (
                    (ws, ys, sig),
                    (jnp.zeros(3), zmax*jnp.ones(3))
                )) @ "measurement"

        return x


    @gen
    def sensor_model(Y, sig, outlier):    

        Y_ = pad(Y, w, val=pad_val)

        I, J = jnp.mgrid[:Y.shape[0], :Y.shape[1]]
        I, J = I.ravel(), J.ravel()
        
        X = genjax.Map(_sensor_model_ij, (0,0,None,None,None))(I,J,Y_,sig,outlier) @ "X"
        X = X.reshape(Y.shape)

        return X

    return sensor_model

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 14
from scipy.stats import truncnorm as scipy_truncnormal

normal_logpdf    = jax.scipy.stats.norm.logpdf
truncnorm_logpdf = jax.scipy.stats.truncnorm.logpdf
truncnorm_pdf    = jax.scipy.stats.truncnorm.pdf

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 17
def get_1d_mixture_components(x, ys, sig, val=-100.0):

    # 1D-Mixture components and value to evaluate.
    # These are given by the distances ALONG ray through `x`
    d  = jnp.linalg.norm(x, axis=-1)
    ds = ys @ x / d
    
    # 1D-Mixture weights.
    # First compute the distances TO ray through `x`
    # and then transforming them appropriately.
    ws = jnp.linalg.norm(ds[...,None] * x/d - ys, axis=-1)
    ws = normal_logpdf(ws, loc=0.0, scale=sig)

    return d, ds, ws

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 21
# TODO: The input Y should be an array only containing range measruements as well. 
#       For this to work we need to have the pixel vectors (the rays through each pixel)

def make_constrained_baseline_model(w, zmax):
    """Returns a untruncated constrained sensor model marginalized over outliers."""    

    pad_val = -100.0

    @genjax.drop_arguments
    @gen
    def _sensor_model_ij(i, j, Y_, sig, outlier):

        # Note that `i,j` are at the edge of the filter window,
        # the Center is offset by `w``
        y  = Y_[i+w,j+w] 
        ys = dslice(Y_, i, j, w).reshape(-1,3)
        
        d, ds, ws = get_1d_mixture_components(y, ys, sig)

        ws = jnp.where(ys[...,2] == pad_val, -jnp.inf, ws)
        ws = ws - logsumexp(ws)

        inlier_outlier_mix = genjax.tfp_mixture(genjax.tfp_categorical, [
                                mixture_of_normals, genjax.tfp_uniform])

        zmax_ = d/y[2]*zmax

        z = inlier_outlier_mix([jnp.log(1.0-outlier), jnp.log(outlier)], (
                                    (ws, ds, sig), 
                                    (0.0, zmax_))) @ "measurement"

        return z * y/d

        
    @gen
    def sensor_model(Y, sig, outlier):   
        """Constrained sensor model."""
        Y_ = pad(Y, w, val=pad_val)

        I, J = jnp.mgrid[:Y.shape[0], :Y.shape[1]]
        I, J = I.ravel(), J.ravel()
                
        
        X = genjax.Map(_sensor_model_ij, (0,0,None,None,None))(I, J, Y_, sig, outlier) @ "X"
        X = X.reshape(Y.shape)

        return X

    return sensor_model

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 23
# TODO: The input Y should be an array only containing range measruements as well. 
#       For this to work we need to have the pixel vectors (the rays through each pixel)

def make_truncated_constrained_baseline_model(w, zmax):
    """Returns a truncated constrained sensor model marginalized over outliers."""    
    pad_val = -100.0

    @genjax.drop_arguments
    @gen
    def _sensor_model_ij(i, j, Y_, sig, outlier):

        # Note that `i,j` are at the edge of the filter window,
        # the Center is offset by `w``
        y  = Y_[i+w,j+w] 
        ys = dslice(Y_, i, j, w).reshape(-1,3)
        
        d, ds, ws = get_1d_mixture_components(y, ys, sig)

        ws = jnp.where(ys[...,2] == pad_val, -jnp.inf, ws)
        ws = ws - logsumexp(ws)


        inlier_outlier_mix = genjax.tfp_mixture(genjax.tfp_categorical, [
                                mixture_of_truncnormals, genjax.tfp_uniform])

        zmax_ = d/y[2]*zmax
        # zmax_ = zmax
        
        z = inlier_outlier_mix([jnp.log(1.0-outlier), jnp.log(outlier)], (
                                    (ws, ds, sig,0.0,zmax_), 
                                    (0.0, zmax_))) @ "measurement"


        return z * y/d

        
    @gen
    def sensor_model(Y, sig, outlier):   
        """Constrained sensor model."""
        Y_ = pad(Y, w, val=pad_val)

        I, J = jnp.mgrid[:Y.shape[0], :Y.shape[1]]
        I, J = I.ravel(), J.ravel()
                
        
        X = genjax.Map(_sensor_model_ij, (0,0,None,None,None))(I, J, Y_, sig, outlier) @ "X"
        X = X.reshape(Y.shape)

        return X

    return sensor_model

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 50
def constrained_lh_mix_ij(i,j, X, Y_padded, zmax, sig, outlier, w):
    x  = X[i, j, :3]
    ys = dslice(Y_padded, i,j, w)

    return constrained_lh_mix(x, ys, zmax=zmax, sig=sig, outlier=outlier)



def constrained_lh(X, Y, zmax, sig, outlier, w:int):
    """"Likelihood of observation X conditioned on Y."""
    Y_   = jax.lax.pad(Y,  -100., ((w,w,0),(w,w,0),(0,0,0)) )
    
    J, I = jnp.meshgrid(jnp.arange(X.shape[1]), jnp.arange(X.shape[0]))
    I = I.ravel()
    J = J.ravel()

    f_ij  = lambda i,j: constrained_lh_mix_ij(i, j, X[:,:,:3], Y_[:,:,:3], zmax=zmax, sig=sig, outlier=outlier, w=w) 
    logps = jax.vmap(f_ij)(I, J)

    return logps.sum()


constrained_lh_jit = jit(constrained_lh,static_argnames=("w",))

# %% ../../scripts/notebooks/_mkl/01 - Likelihood Baselines.ipynb 58
from genjax.generative_functions.distributions import ExactDensity

class B3DImageLikelihood(ExactDensity):
    def sample(self, key, Y, zmax, sig, outlier, w:int):
        # TODO: write this method
        return Y

    def logpdf(self, X, Y, zmax, sig, outlier, w:int):
        return constrained_lh_jit(X, Y, zmax, sig, outlier, w)

b3d_image_likelihood = B3DImageLikelihood()

