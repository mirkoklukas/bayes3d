var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"homepage.html","title":"Overview","text":"<p>GenJAX: a probabilistic programming library designed to scale probabilistic modeling and inference into high performance settings. (1)</p> <ol> <li> <p>Here, high performance means massively parallel, either cores or devices.</p> <p>For those whom this overview page may be irrelevant: the value proposition is about putting expressive models and customizable Bayesian inference on GPUs, TPUs, etc - without sacrificing abstraction or modularity.</p> </li> </ol> <p>Gen is a multi-paradigm (generative, differentiable, incremental) system for probabilistic programming. GenJAX is an implementation of Gen on top of JAX (2) - exposing the ability to programmatically construct and manipulate generative functions (1) (computational objects which represent probability measures over structured sample spaces), with compilation to native devices, accelerators, and other parallel fabrics. </p> <ol> <li> <p>By design, generative functions expose a concise interface for expressing approximate and differentiable inference algorithms. </p> <p>The set of generative functions is extensible! You can implement your own - allowing advanced users to performance optimize their critical modeling/inference code paths.</p> <p>You can (and we, at the MIT Probabilistic Computing Project, do!) use these objects for machine learning - including robotics, natural language processing, reasoning about agents, and modelling / creating systems which exhibit human-like reasoning.</p> <p>A precise mathematical formulation of generative functions is given in Marco Cusumano-Towner's PhD thesis.</p> </li> <li> <p>If the usage of JAX is not a dead giveaway, GenJAX is written in Python.</p> </li> </ol> Model codeInference code <p><p> Defining a beta-bernoulli process model as a generative function in GenJAX. </p></p> <pre><code>@genjax.gen\ndef model():\np = beta(0, 1) @ \"p\"\nv = bernoulli(p) @ \"v\"\nreturn v\n</code></pre> <p><p> This works for any generative function, not just the beta-bernoulli model. </p></p> <pre><code>def importance_sampling(\nkey: PRNGKey,\ngen_fn: GenerativeFunction,\nmodel_args: Tuple,\nobs: ChoiceMap,\nn_samples: Int,\n): # (1)!\nkey, sub_keys = genjax.slash(key, n_samples)  # split keys\n_, (lws, trs) = jax.vmap(\ngen_fn.importance, # (2)!\nin_axes=(0, None, None),\n)(sub_keys, obs, args)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (trs, log_normalized_weights, log_ml_estimate)\n</code></pre> <ol> <li> <p>Here's a few notes about the signature:</p> <ul> <li><code>PRNGKey</code> is the type of <code>jax.random.PRNGKey</code>. In GenJAX, we pass keys into generative code, and generative code returns a changed key.</li> <li><code>GenerativeFunction</code> refers to generative functions, objects which expose Gen's probabilistic interface.</li> <li>For now, think of <code>ChoiceMap</code> as the type of object which Gen uses to express conditioning.</li> </ul> </li> <li> <p><code>gen_fn.importance</code> is a generative function interface method. Generative functions are responsible for implementing this method, to support conditional sampling and conditional density estimation. You can learn a lot more about this method in the generative function interface.</p> </li> </ol>"},{"location":"homepage.html#what-sort-of-things-do-you-use-genjax-for","title":"What sort of things do you use GenJAX for?","text":"Real time object tracking <p>Real time tracking of objects in 3D using probabilistic rendering. (Left) Ground truth, (center) depth mask, (right) inference overlaid on ground truth.</p> <p><p> </p></p>"},{"location":"homepage.html#why-gen","title":"Why Gen?","text":"<p>GenJAX is a Gen implementation. If you're considering using GenJAX - it's worth starting by understanding what problems Gen purports to solve.</p>"},{"location":"homepage.html#the-evolution-of-probabilistic-programming-languages","title":"The evolution of probabilistic programming languages","text":"<p>Probabilistic modeling and inference is hard: understanding a domain well enough to construct a probabilistic model in the Bayesian paradigm is challenging, and that's half the battle - the other half is designing effective inference algorithms to probe the implications of the model (1).</p> <ol> <li> <p>Some probabilistic programming languages restrict the set of allowable models, providing (in return) efficient (often, exact) inference. </p> <p>Gen considers a wide class of models - include Bayesian nonparametrics, open-universe models, and models over rich structures (like programs!) - which don't natively support efficient exact inference.</p> </li> </ol> <p>Model writers have historically considered the following design loop.</p> <pre><code>graph LR\n  A[Design model.] --&gt; B[Implement inference by hand.];\n  B --&gt; C[Model + inference okay?];\n  C --&gt; D[Happy.];\n  C --&gt; A;</code></pre> <p>The first generation (1) of probabilistic programming systems introduced inference engines which could operate abstractly over many different models, without requiring the programmer to return and tweak their inference code. The utopia envisioned by these systems is shown below.</p> <ol> <li> <p>Here, the definition of \"first generation\" includes systems like JAGS, BUGS, BLOG, IBAL, Church, Infer.NET, Figaro, Stan, amongst others.</p> <p>But more precisely, many systems preceded the DARPA PPAML project - which gave rise to several novel systems, including the predecessors of Gen.</p> </li> </ol> <pre><code>graph LR\n  A[Design model.] --&gt; D[Model + inference okay?];\n  B[Inference engine.] ---&gt; D;\n  D --&gt; E[Happy.];\n  D ---&gt; A;</code></pre> <p>The problem with this utopia is that we often need to customize our inference algorithms (1) to achieve maximum performance, with respect to accuracy as well as runtime (2). First generation systems were not designed with this in mind.</p> <ol> <li>Here, programmable inference denotes using a custom proposal distribution in importance sampling, or a custom variational family for variational inference, or even a custom kernel in Markov chain Monte Carlo.</li> <li>Composition of inference programs can also be highly desirable when performing inference in complex models, or designing a probabilistic application from several modeling and inference components. The first examples of universal inference engines ignored this design problem.</li> </ol>"},{"location":"homepage.html#programmable-inference","title":"Programmable inference","text":"<p>A worthy design goal is to allow users to customize when required, while retaining the rapid model/inference iteration properties explored by first generation systems.</p> <p>Gen addresses this goal by introducing a separation between modeling and inference code: the generative function interface.</p> <p> </p> <p>The interface provides an abstraction layer that inference algorithms can call to compute the necessary (and hard to get right!) math (1). Probabilistic application developers can also extend the interface to new modeling languages - and immediately gain access to advanced inference procedures.</p> <ol> <li> <p>Examples of hard-to-get-right math: importance weights, accept reject ratios, and gradient estimators. </p> <p>For simple models and inference, one might painlessly derive these quantities. As soon as the model/inference gets complicated, however, you might find yourself thanking the interface.</p> </li> </ol>"},{"location":"homepage.html#whose-using-gen","title":"Whose using Gen?","text":"<p>Gen supports a growing list of users, with collaboration across academic research labs and industry affiliates.</p> <p> </p> <p>We're looking to expand our user base! If you're interested, please contact us to get involved.</p>"},{"location":"genjax/diff_jl.html","title":"Comparisons with Gen.jl","text":"<p>GenJAX implements concepts from Gen, and implements several inference algorithms with reference to implementations from <code>Gen.jl</code>. In general, you should find that the programming patterns and interface idioms should match closely with <code>Gen.jl</code>.</p> <p>However, there are a few necessary design deviations between <code>genjax</code> and <code>Gen.jl</code> that stem from restrictions arising from JAX's compilation model. In this section, we describe several of these differences and try to highlight workarounds or discuss the reason for the discrepancy.</p>"},{"location":"genjax/diff_jl.html#turing-universality","title":"Turing universality","text":"<p><code>Gen.jl</code> is Turing universal - it can encode any computable distribution, including those expressed by forms of unbounded recursion.</p> <p>Arguing from a practical perspective, <code>genjax</code> also falls into this category, but several things are harder to encode for GPUs(1). Additionally, JAX does not feature mechanisms for dynamic shape allocations, but it does feature mechanisms for unbounded recursion.</p> <ol> <li>We expect that GPU/TPU deployment to be the dominant usage pattern for <code>genjax</code> - and defer optimized CPU deployment to other implementations of Gen.</li> </ol> <p>Lack of dynamic allocations provides a technical barrier to implementing Gen's trace machinery for generative functions which feature recursive calls to other generative functions. While JAX allows for unbounded recursion, to generally support recording trace data - we also need the ability to dynamically allocate choice data. This requirement is currently at tension with XLA's requirements of knowing the static shape of everything. Nonetheless, one might imagine pre-allocating large arrays - passing them into <code>jax.lax.while_op</code> implementations of certain types of recursion, etc. Painful and impractical - yes, but theoretical possible.</p> <p><code>genjax</code> supports generative function combinators with bounded recursion / unfold chain length. Ahead of time, these combinators can be directed to pre-allocate arrays with enough size to handle recursion/looping within the bounds that the programmer sets. If these bounds are exceeded, a Python runtime error will be thrown (both on and off JAX device).</p> <p>In practice, this means that some performance engineering (space vs. expressivity) is required of the programmer. It's certainly feasible to express bounded recursive computations which terminate with probability 1 - but you'll need to ahead of time allocate space for it.</p>"},{"location":"genjax/diff_jl.html#mutation","title":"Mutation","text":"<p>Just like JAX, GenJAX disallows mutation - expressing a mutating operation on an array must be done through special JAX interfaces. Outside of JIT compilation, those interfaces often fully copy array data. Inside of JIT compilation, there are special circumstances where these operations will be performed in place.</p>"},{"location":"genjax/diff_jl.html#to-jit-or-not-to-jit","title":"To JIT or not to JIT","text":"<p><code>Gen.jl</code> is written in Julia, which automatically JITs everything. <code>genjax</code>, by virtue of being constructed on top of JAX, allows us to JIT JAX compatible code - but the JIT process is user directed. Thus, the idioms that are used to express and optimize inference code are necessarily different compared to <code>Gen.jl</code>. In the inference standard library, you'll typically find algorithms implemented as dataclasses which inherit (and implement) the <code>jax.Pytree</code> interfaces. Implementing these interfaces allow usage of inference dataclasses and methods in jittable code - and, as a bonus, allow us to be specific about trace vs. runtime known values.</p> <p>In general, it's productive to enclose as much of a computation as possible in a <code>jax.jit</code> block. This can sometimes lead to long trace times. If trace times are ballooning, a common source is explicit for-loops (with known bounds, else JAX will complain). In these cases, you might look at Advice on speeding up compilation time. We've taken care to optimize (by e.g. using XLA primitives) the code which we expose from GenJAX - but if you find something out of the ordinary, file an issue!</p>"},{"location":"genjax/language_aperitifs.html","title":"Language ap\u00e9ritifs","text":"<p>This page assumes that the reader has familiarity with trace-based probabilistic programming systems.</p> <p>The implementation of GenJAX adhers to commonly accepted JAX idioms (1) and modern functional programming patterns (2).</p> <ol> <li>One example: everything is a Pytree. Implies another: everything is JAX traceable by default.</li> <li>Modern here meaning patterns concerning the composition of effectful computations via effect handling abstractions.</li> </ol> <p>GenJAX consists of a set of languages based around transforming pure functions to apply semantic transformations. On this page, we'll provide a taste of some of these languages.</p>"},{"location":"genjax/language_aperitifs.html#the-builtin-language","title":"The builtin language","text":"<p>GenJAX provides a builtin language which supports a <code>trace</code> primitive and the ability to invoke other generative functions as callees:</p> <pre><code>@genjax.gen\ndef submodel():\nx = trace(\"x\", normal)(0.0, 1.0) # explicit\nreturn x\n@genjax.gen\ndef model():\nx = submodel() @ \"sub\" # sugared\nreturn x\n</code></pre> <p>The <code>trace</code> call is a JAX primitive which is given semantics by transformations which implement the semantics of inference interfaces described in Generative functions.</p> <p>Addresses (here, <code>\"x\"</code> and <code>\"sub\"</code>) are important - addressed random choices within <code>trace</code> allow us to structure the address hierarchy for the measure over choice maps which generative functions in this language define.</p> <p>Because convenient idioms for working with addresses is so important in Gen, the generative functions from the builtin language also support a form of \"splatting\" addresses into a caller.</p> <pre><code>@genjax.gen\ndef model():\nx = submodel.inline()\nreturn x\n</code></pre> <p>Invoking the <code>submodel</code> via the <code>inline</code> interface here means that the addresses in <code>submodel</code> are flattened into the address level for the <code>model</code>. If there's overlap, that's a problem! But GenJAX will yell at you for that.</p>"},{"location":"genjax/language_aperitifs.html#structured-control-flow-with-combinators","title":"Structured control flow with combinators","text":"<p>The base modeling language is the <code>BuiltinGenerativeFunction</code> language shown above. The builtin language is based on pure functions, with the interface semantics implemented using program transformations. But we'd also like to take advantage of structured control flow in our generative computations. </p> <p>Users gain access to structured control flow via combinators, other generative function mini-languages which implement the interfaces in control flow compatible ways.</p> <pre><code>@functools.partial(genjax.Map, in_axes=(0, 0))\n@genjax.gen\ndef kernel(x, y):\nz = normal(x + y, 1.0) @ \"z\"\nreturn z\n</code></pre> <p>This defines a <code>MapCombinator</code> generative function - a generative function whose interfaces take care of applying <code>vmap</code> in the appropriate ways (1).</p> <ol> <li>Read: compatible with JIT, gradients, and incremental computation.</li> </ol> <p><code>MapCombinator</code> has a vectorial friend named <code>UnfoldCombinator</code> which implements a <code>scan</code>-like pattern of generative computation.</p> <pre><code>@functools.partial(genjax.Unfold, max_length = 10)\n@genjax.gen\ndef scanner(prev, static_args):\nsigma, = static_args\nnew = normal(prev, sigma) @ \"z\"\nreturn new\n</code></pre> <p><code>UnfoldCombinator</code> allows the expression of general state space models - modeled as a generative function which supports a dependent-for (1) control flow pattern.</p> <ol> <li>Dependent-for means that each iteration may depend on the output from the previous iteration. Think of <code>jax.lax.scan</code> here.</li> </ol> <p><code>UnfoldCombinator</code> allows uncertainty over the length of the chain:</p> <pre><code>@genjax.gen\ndef top_model(p):\nlength = truncated_geometric(10, p) @ \"l\"\ninitial_state = normal(0.0, 1.0) @ \"init\"\nsigma = normal(0.0, 1.0) @ \"sigma\"\n(v, xs) = scanner(length, initial_state, sigma)\nreturn v\n</code></pre> <p>Here, <code>length</code> is drawn from a truncated geometric distribution, and determines the index range of the chain which participates in the generative computation.</p> <p>Of course, combinators are composable.</p> <pre><code>@functools.partial(genjax.Map, in_axes = (0, ))\n@genjax.gen\ndef top_model(p):\nlength = truncated_geometric(10, p) @ \"l\"\ninitial_state = normal(0.0, 1.0) @ \"init\"\nsigma = normal(0.0, 1.0) @ \"sigma\"\n(v, xs) = scanner(length, initial_state, sigma)\nreturn v\n</code></pre> <p>Now we're describing a broadcastable generative function whose internal choices include a chain-like generative structure with dynamic truncation using padding. And we could go on!</p>"},{"location":"genjax/notebooks.html","title":"Modeling &amp; inference notebooks","text":"<p>Link to the notebook repository</p> <p>This section contains a link to a (statically hosted) series of tutorial notebooks designed to guide usage of GenJAX. These notebooks are executed and rendered with quarto, and are kept up to date with the repository along with the documentation.</p> <p>The notebook repository can be found here.</p>"},{"location":"genjax/concepts/generative_functions.html","title":"Generative functions","text":"<p>Gen is all about generative functions: computational objects which support an interface that helps automate the tricky math involved in programming Bayesian inference algorithms. In this section, we'll unpack the generative function interface and explain the mathematics behind generative functions (1).</p> <ol> <li>For a deeper dive, enjoy Marco Cusumano-Towner's PhD thesis.</li> </ol>"},{"location":"genjax/library/index.html","title":"Library reference","text":"<p>This is the API documentation for modules and symbols which are exposed publicly from <code>genjax</code>. </p> <p>The <code>genjax</code> package consists of several modules, many of which rely on functionality from the <code>genjax.core</code> module, and build upon datatypes, transforms, and generative datatypes which are documented there. Generative function languages use the core datatypes and transforms to implement the generative function interface. Inference and learning algorithms are then implemented using the interface.</p> <ul> <li>The core documentation discusses key datatypes and transformations, which are used throughout the codebase.</li> <li>The documentation on generative function languages describes the functionality and usage for several generative function implementations, including distributions, a function-like language with primitives that allow callee generative functions, and combinator languages which provide structured patterns of control flow.</li> </ul> <ul> <li>The documentation on extension modules describes how users can extend GenJAX with new generative functions and inference functionality, while depending on 3rd party libraries.</li> </ul>"},{"location":"genjax/library/core/index.html","title":"Core","text":""},{"location":"genjax/library/core/index.html#genjax._src.core","title":"<code>genjax._src.core</code>","text":"<p>This module provides the core functionality and JAX compatibility layer which <code>GenJAX</code> generative function and inference modules are built on top of. It contains (truncated, and in no particular order):</p> <ul> <li> <p>Core data types for the associated data types of generative functions.</p> </li> <li> <p>Utility abstract data types (mixins) for automatically registering class definitions as valid <code>Pytree</code> method implementors (guaranteeing <code>flatten</code>/<code>unflatten</code> compatibility across JAX transform boundaries). For more information, see Pytrees.</p> </li> <li> <p>Transformation interpreters: interpreter-based transformations which operate on <code>ClosedJaxpr</code> instances, as well as staging functionality for staging out computations to <code>ClosedJaxpr</code> instances. The core interpreters are written in a mixed initial / final style. The application of all interpreters are JAX compatible, meaning that the application of any interpreter can be staged out to eliminate the interpreter overhead.</p> </li> </ul>"},{"location":"genjax/library/core/datatypes.html","title":"Core datatypes","text":"<p>GenJAX features a set of core abstract datatypes which build on JAX's <code>Pytree</code> interface. These datatypes are used as an abstract base mixin (especially GenJAX's <code>Pytree</code> utility abstract base class) for basically all of the dataclasses in GenJAX.</p>"},{"location":"genjax/library/core/datatypes.html#pytree","title":"Pytree","text":""},{"location":"genjax/library/core/datatypes.html#genjax.core.Pytree","title":"<code>genjax.core.Pytree</code>","text":"<p>Abstract base class which registers a class with JAX's <code>Pytree</code> system.</p> <p>Users who mixin this ABC for class definitions are required to implement <code>flatten</code> below. In turn, instances of the class gain access to a large set of utility functions for working with <code>Pytree</code> data, as well as the ability to use <code>jax.tree_util</code> Pytree functionality.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>class Pytree(metaclass=abc.ABCMeta):\n\"\"\"&gt; Abstract base class which registers a class with JAX's `Pytree`\n    system.\n    Users who mixin this ABC for class definitions are required to\n    implement `flatten` below. In turn, instances of the class gain\n    access to a large set of utility functions for working with `Pytree`\n    data, as well as the ability to use `jax.tree_util` Pytree\n    functionality.\n    \"\"\"\ndef __init_subclass__(cls, **kwargs):\nsuper().__init_subclass__(**kwargs)\njtu.register_pytree_node(\ncls,\ncls.flatten,\ncls.unflatten,\n)\n@abc.abstractmethod\ndef flatten(self) -&gt; Tuple[Tuple, Tuple]:\n\"\"\"`flatten` must be implemented when a user mixes `Pytree` into the\n        declaration of a new class or dataclass.\n        The implementation of `flatten` assumes the following contract:\n        * must return a 2-tuple of tuples.\n        * the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.\n        * the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for `Pytree` equality comparison.\n        For more information, consider [JAX's documentation on Pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n        Returns:\n            dynamic: Dynamic data which supports JAX tracer values.\n            static: Static data which is JAX trace time constant.\n        Examples:\n            Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the `Pytree` mixin.\n            ```python\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            ```\n            !!! info \"Ordering fields in `Pytree` declarations\"\n                Note that the ordering in the dataclass declaration **does matter** - you should put static fields first. The automatically defined `unflatten` method (c.f. below) assumes this ordering.\n            Now, given the declaration, you can use `jax.tree_util` flattening/unflatten functionality.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import genjax\n            import jax.tree_util as jtu\n            from genjax.core import Pytree\n            from dataclasses import dataclass\n            console = genjax.pretty()\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            f = MyFoo(0, 1.0)\n            leaves, form = jtu.tree_flatten(f)\n            print(console.render(leaves))\n            new = jtu.tree_unflatten(form, leaves)\n            print(console.render(new))\n            ```\n        \"\"\"\n@classmethod\ndef unflatten(cls, data, xs):\n\"\"\"Given an implementation of `flatten` (c.f. above), `unflatten` is\n        automatically defined and registered with JAX's `Pytree` system.\n        `unflatten` allows usage of `jtu.tree_unflatten` to create instances of a declared class that mixes `Pytree` from a `PyTreeDef` for that class and leaf data.\n        Examples:\n            Our example from `flatten` above also applies here - where we use `jtu.tree_unflatten` to create a new instance of `MyFoo` from a `PyTreeDef` and leaf data.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import genjax\n            import jax.tree_util as jtu\n            from genjax.core import Pytree\n            from dataclasses import dataclass\n            console = genjax.pretty()\n            @dataclass\n            class MyFoo(Pytree):\n                static_field: Any\n                dynamic_field: Any\n                # Implementing `flatten`\n                def flatten(self):\n                    return (self.dynamic_field, ), (self.static_field, )\n            f = MyFoo(0, 1.0)\n            leaves, form = jtu.tree_flatten(f)\n            new = jtu.tree_unflatten(form, leaves)\n            print(console.render(new))\n            ```\n        \"\"\"\nreturn cls(*data, *xs)\n@classmethod\ndef new(cls, *args, **kwargs):\nreturn cls(*args, **kwargs)\n# This exposes slicing the struct-of-array representation,\n# taking leaves and indexing/randing into them on the first index,\n# returning a value with the same `Pytree` structure.\ndef slice(self, index_or_index_array):\n\"\"\"&gt; Utility available to any class which mixes `Pytree` base. This\n        method supports indexing/slicing on indices when leaves are arrays.\n        `obj.slice(index)` will take an instance whose class extends `Pytree`, and return an instance of the same class type, but with leaves indexed into at `index`.\n        Arguments:\n            index_or_index_array: An `Int` index or an array of indices which will be used to index into the leaf arrays of the `Pytree` instance.\n        Returns:\n            new_instance: A `Pytree` instance of the same type, whose leaf values are the results of indexing into the leaf arrays with `index_or_index_array`.\n        \"\"\"\nreturn jtu.tree_map(lambda v: v[index_or_index_array], self)\ndef stack(self, *trees):\nreturn tree_stack([self, *trees])\ndef unstack(self):\nreturn tree_unstack(self)\n# Lift multiple trees into a sum type.\ndef sum(self, *trees):\nreturn Sumtree.new(self, trees)\n# Defines default pretty printing.\ndef __rich_console__(self, console, options):\ntree = gpp.tree_pformat(self)\nyield tree\ndef __rich_repr__(self):\nyield self\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.flatten","title":"<code>flatten()</code>  <code>abstractmethod</code>","text":"<p><code>flatten</code> must be implemented when a user mixes <code>Pytree</code> into the declaration of a new class or dataclass.</p> <p>The implementation of <code>flatten</code> assumes the following contract:</p> <ul> <li>must return a 2-tuple of tuples.</li> <li>the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.</li> <li>the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for <code>Pytree</code> equality comparison.</li> </ul> <p>For more information, consider JAX's documentation on Pytrees.</p> <p>Returns:</p> Name Type Description <code>dynamic</code> <code>Tuple</code> <p>Dynamic data which supports JAX tracer values.</p> <code>static</code> <code>Tuple</code> <p>Static data which is JAX trace time constant.</p> <p>Examples:</p> <p>Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the <code>Pytree</code> mixin.</p> <pre><code>@dataclass\nclass MyFoo(Pytree):\nstatic_field: Any\ndynamic_field: Any\n# Implementing `flatten`\ndef flatten(self):\nreturn (self.dynamic_field, ), (self.static_field, )\n</code></pre> <p>Ordering fields in <code>Pytree</code> declarations</p> <p>Note that the ordering in the dataclass declaration does matter - you should put static fields first. The automatically defined <code>unflatten</code> method (c.f. below) assumes this ordering.</p> <p>Now, given the declaration, you can use <code>jax.tree_util</code> flattening/unflatten functionality.</p> SourceResult <pre><code>import genjax\nimport jax.tree_util as jtu\nfrom genjax.core import Pytree\nfrom dataclasses import dataclass\nconsole = genjax.pretty()\n@dataclass\nclass MyFoo(Pytree):\nstatic_field: Any\ndynamic_field: Any\n# Implementing `flatten`\ndef flatten(self):\nreturn (self.dynamic_field, ), (self.static_field, )\nf = MyFoo(0, 1.0)\nleaves, form = jtu.tree_flatten(f)\nprint(console.render(leaves))\nnew = jtu.tree_unflatten(form, leaves)\nprint(console.render(new))\n</code></pre> <p><pre><code>[1.0]\n</code></pre> <pre><code>MyFoo\n\u251c\u2500\u2500 static_field\n\u2502   \u2514\u2500\u2500 (const) 0\n\u2514\u2500\u2500 dynamic_field\n    \u2514\u2500\u2500 (const) 1.0\n</code></pre></p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@abc.abstractmethod\ndef flatten(self) -&gt; Tuple[Tuple, Tuple]:\n\"\"\"`flatten` must be implemented when a user mixes `Pytree` into the\n    declaration of a new class or dataclass.\n    The implementation of `flatten` assumes the following contract:\n    * must return a 2-tuple of tuples.\n    * the first tuple is \"dynamic\" data - things that JAX tracers are allowed to population.\n    * the second tuple is \"static\" data - things which are known at JAX tracing time. Static data is also used by JAX for `Pytree` equality comparison.\n    For more information, consider [JAX's documentation on Pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n    Returns:\n        dynamic: Dynamic data which supports JAX tracer values.\n        static: Static data which is JAX trace time constant.\n    Examples:\n        Let's assume that you are implementing a new dataclass. Here's how you would define the dataclass using the `Pytree` mixin.\n        ```python\n        @dataclass\n        class MyFoo(Pytree):\n            static_field: Any\n            dynamic_field: Any\n            # Implementing `flatten`\n            def flatten(self):\n                return (self.dynamic_field, ), (self.static_field, )\n        ```\n        !!! info \"Ordering fields in `Pytree` declarations\"\n            Note that the ordering in the dataclass declaration **does matter** - you should put static fields first. The automatically defined `unflatten` method (c.f. below) assumes this ordering.\n        Now, given the declaration, you can use `jax.tree_util` flattening/unflatten functionality.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import genjax\n        import jax.tree_util as jtu\n        from genjax.core import Pytree\n        from dataclasses import dataclass\n        console = genjax.pretty()\n        @dataclass\n        class MyFoo(Pytree):\n            static_field: Any\n            dynamic_field: Any\n            # Implementing `flatten`\n            def flatten(self):\n                return (self.dynamic_field, ), (self.static_field, )\n        f = MyFoo(0, 1.0)\n        leaves, form = jtu.tree_flatten(f)\n        print(console.render(leaves))\n        new = jtu.tree_unflatten(form, leaves)\n        print(console.render(new))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.unflatten","title":"<code>unflatten(data, xs)</code>  <code>classmethod</code>","text":"<p>Given an implementation of <code>flatten</code> (c.f. above), <code>unflatten</code> is automatically defined and registered with JAX's <code>Pytree</code> system.</p> <p><code>unflatten</code> allows usage of <code>jtu.tree_unflatten</code> to create instances of a declared class that mixes <code>Pytree</code> from a <code>PyTreeDef</code> for that class and leaf data.</p> <p>Examples:</p> <p>Our example from <code>flatten</code> above also applies here - where we use <code>jtu.tree_unflatten</code> to create a new instance of <code>MyFoo</code> from a <code>PyTreeDef</code> and leaf data.</p> SourceResult <pre><code>import genjax\nimport jax.tree_util as jtu\nfrom genjax.core import Pytree\nfrom dataclasses import dataclass\nconsole = genjax.pretty()\n@dataclass\nclass MyFoo(Pytree):\nstatic_field: Any\ndynamic_field: Any\n# Implementing `flatten`\ndef flatten(self):\nreturn (self.dynamic_field, ), (self.static_field, )\nf = MyFoo(0, 1.0)\nleaves, form = jtu.tree_flatten(f)\nnew = jtu.tree_unflatten(form, leaves)\nprint(console.render(new))\n</code></pre> <pre><code>MyFoo\n\u251c\u2500\u2500 static_field\n\u2502   \u2514\u2500\u2500 (const) 0\n\u2514\u2500\u2500 dynamic_field\n    \u2514\u2500\u2500 (const) 1.0\n</code></pre> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@classmethod\ndef unflatten(cls, data, xs):\n\"\"\"Given an implementation of `flatten` (c.f. above), `unflatten` is\n    automatically defined and registered with JAX's `Pytree` system.\n    `unflatten` allows usage of `jtu.tree_unflatten` to create instances of a declared class that mixes `Pytree` from a `PyTreeDef` for that class and leaf data.\n    Examples:\n        Our example from `flatten` above also applies here - where we use `jtu.tree_unflatten` to create a new instance of `MyFoo` from a `PyTreeDef` and leaf data.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import genjax\n        import jax.tree_util as jtu\n        from genjax.core import Pytree\n        from dataclasses import dataclass\n        console = genjax.pretty()\n        @dataclass\n        class MyFoo(Pytree):\n            static_field: Any\n            dynamic_field: Any\n            # Implementing `flatten`\n            def flatten(self):\n                return (self.dynamic_field, ), (self.static_field, )\n        f = MyFoo(0, 1.0)\n        leaves, form = jtu.tree_flatten(f)\n        new = jtu.tree_unflatten(form, leaves)\n        print(console.render(new))\n        ```\n    \"\"\"\nreturn cls(*data, *xs)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.slice","title":"<code>slice(index_or_index_array)</code>","text":"<p>Utility available to any class which mixes <code>Pytree</code> base. This method supports indexing/slicing on indices when leaves are arrays.</p> <p><code>obj.slice(index)</code> will take an instance whose class extends <code>Pytree</code>, and return an instance of the same class type, but with leaves indexed into at <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index_or_index_array</code> <p>An <code>Int</code> index or an array of indices which will be used to index into the leaf arrays of the <code>Pytree</code> instance.</p> required <p>Returns:</p> Name Type Description <code>new_instance</code> <p>A <code>Pytree</code> instance of the same type, whose leaf values are the results of indexing into the leaf arrays with <code>index_or_index_array</code>.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def slice(self, index_or_index_array):\n\"\"\"&gt; Utility available to any class which mixes `Pytree` base. This\n    method supports indexing/slicing on indices when leaves are arrays.\n    `obj.slice(index)` will take an instance whose class extends `Pytree`, and return an instance of the same class type, but with leaves indexed into at `index`.\n    Arguments:\n        index_or_index_array: An `Int` index or an array of indices which will be used to index into the leaf arrays of the `Pytree` instance.\n    Returns:\n        new_instance: A `Pytree` instance of the same type, whose leaf values are the results of indexing into the leaf arrays with `index_or_index_array`.\n    \"\"\"\nreturn jtu.tree_map(lambda v: v[index_or_index_array], self)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.stack","title":"<code>stack(*trees)</code>","text":"Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def stack(self, *trees):\nreturn tree_stack([self, *trees])\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.pytree.Pytree.unstack","title":"<code>unstack()</code>","text":"Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def unstack(self):\nreturn tree_unstack(self)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#abstract-base-classes-which-extend-pytree","title":"Abstract base classes which extend <code>Pytree</code>","text":""},{"location":"genjax/library/core/datatypes.html#genjax.core.Tree","title":"<code>genjax.core.Tree</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>The <code>Tree</code> class is used to define abstract classes for tree-shaped datatypes. These classes are used to implement trace, choice map, and selection types.</p> <p>One should think of <code>Tree</code> as providing a convenient base class for many of the generative datatypes declared in GenJAX. <code>Tree</code> mixes in <code>Pytree</code> automatically.</p> Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@dataclass\nclass Tree(Pytree):\n\"\"\"&gt; The `Tree` class is used to define abstract classes for tree-shaped\n    datatypes. These classes are used to implement trace, choice map, and\n    selection types.\n    One should think of `Tree` as providing a convenient base class for\n    many of the generative datatypes declared in GenJAX. `Tree` mixes in\n    `Pytree` automatically.\n    \"\"\"\n@abc.abstractmethod\ndef has_subtree(self, addr) -&gt; bool:\npass\n@abc.abstractmethod\ndef get_subtree(self, addr):\npass\n@abc.abstractmethod\ndef get_subtrees_shallow(self):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Tree.has_subtree","title":"<code>has_subtree(addr)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef has_subtree(self, addr) -&gt; bool:\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Tree.get_subtree","title":"<code>get_subtree(addr)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef get_subtree(self, addr):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Tree.get_subtrees_shallow","title":"<code>get_subtrees_shallow()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef get_subtrees_shallow(self):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax.core.Leaf","title":"<code>genjax.core.Leaf</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Tree</code></p> <p>The <code>Leaf</code> class specializes <code>Tree</code> to classes without any internal subtrees.</p> <p><code>Leaf</code> is a convenient base for generative datatypes which don't keep reference to other <code>Tree</code> instances - things like <code>ValueChoiceMap</code> (whose only choice value is a single value, not a dictionary or other tree-like object). <code>Leaf</code> extends <code>Tree</code> with a special extension method <code>get_leaf_value</code>.</p> Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@dataclass\nclass Leaf(Tree):\n\"\"\"&gt; The `Leaf` class specializes `Tree` to classes without any internal\n    subtrees.\n    `Leaf` is a convenient base for generative datatypes which don't keep reference to other `Tree` instances - things like `ValueChoiceMap` (whose only choice value is a single value, not a dictionary or other tree-like object). `Leaf` extends `Tree` with a special extension method `get_leaf_value`.\n    \"\"\"\n@abc.abstractmethod\ndef get_leaf_value(self):\npass\n@abc.abstractmethod\ndef set_leaf_value(self, v):\npass\ndef has_subtree(self, addr):\nreturn False\ndef get_subtree(self, addr):\nraise Exception(\nf\"{type(self)} is a Leaf: it does not address any internal choices.\"\n)\ndef get_subtrees_shallow(self):\nraise Exception(f\"{type(self)} is a Leaf: it does not have any subtrees.\")\ndef merge(self, other):\nraise Exception(f\"{type(self)} is a Leaf: can't merge.\")\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.get_leaf_value","title":"<code>get_leaf_value()</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>@abc.abstractmethod\ndef get_leaf_value(self):\npass\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.has_subtree","title":"<code>has_subtree(addr)</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>def has_subtree(self, addr):\nreturn False\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.get_subtree","title":"<code>get_subtree(addr)</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>def get_subtree(self, addr):\nraise Exception(\nf\"{type(self)} is a Leaf: it does not address any internal choices.\"\n)\n</code></pre>"},{"location":"genjax/library/core/datatypes.html#genjax._src.core.datatypes.tree.Leaf.get_subtrees_shallow","title":"<code>get_subtrees_shallow()</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/tree.py</code> <pre><code>def get_subtrees_shallow(self):\nraise Exception(f\"{type(self)} is a Leaf: it does not have any subtrees.\")\n</code></pre>"},{"location":"genjax/library/core/generative.html","title":"Generative datatypes","text":"<p>Key generative datatypes in Gen</p> <p>This documentation page contains the type and interface documentation for the primary generative datatypes used in Gen. The documentation on this page deals with the abstract base classes for these datatypes. </p> <p>Any concrete implementor of these abstract classes should be documented with the language which implements it.</p>"},{"location":"genjax/library/core/generative.html#generative-functions","title":"Generative functions","text":"<p>The main computational objects in Gen are generative functions. These objects support an abstract interface of methods and associated types. The interface is designed to allow inference layers to abstract over implementations.</p> <p>Below, we document the abstract base class, and illustrate example usage using concrete implementors. Full descriptions of concrete generative function languages are described in their own documentation module.</p>"},{"location":"genjax/library/core/generative.html#genjax.core.GenerativeFunction","title":"<code>genjax.core.GenerativeFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Abstract base class for generative functions.</p> <p>Interaction with JAX</p> <p>Concrete implementations of <code>GenerativeFunction</code> will likely interact with the JAX tracing machinery if used with the languages exposed by <code>genjax</code>. Hence, there are specific implementation requirements which are more stringent than the requirements enforced in other Gen implementations (e.g. Gen in Julia).</p> <ul> <li>For broad compatibility, the implementation of the interfaces should be compatible with JAX tracing.</li> <li>If a user wishes to implement a generative function which is not compatible with JAX tracing, that generative function may invoke other JAX compat generative functions, but likely cannot be invoked inside of JAX compat generative functions.</li> </ul> <p>Aside from JAX compatibility, an implementor should match the interface signatures documented below. This is not statically checked - but failure to do so will lead to unintended behavior or errors.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass GenerativeFunction(Pytree):\n\"\"\"&gt; Abstract base class for generative functions.\n    !!! info \"Interaction with JAX\"\n        Concrete implementations of `GenerativeFunction` will likely interact with the JAX tracing machinery if used with the languages exposed by `genjax`. Hence, there are specific implementation requirements which are more stringent than the requirements\n        enforced in other Gen implementations (e.g. Gen in Julia).\n        * For broad compatibility, the implementation of the interfaces *should* be compatible with JAX tracing.\n        * If a user wishes to implement a generative function which is not compatible with JAX tracing, that generative function may invoke other JAX compat generative functions, but likely cannot be invoked inside of JAX compat generative functions.\n    Aside from JAX compatibility, an implementor *should* match the interface signatures documented below. This is not statically checked - but failure to do so\n    will lead to unintended behavior or errors.\n    \"\"\"\n# This is used to support tracing -- the user is not required to provide\n# a PRNGKey, because the value of the key is not important, only\n# the fact that the value has type PRNGKey.\ndef __abstract_call__(self, *args) -&gt; Tuple[PRNGKey, Any]:\nkey = jax.random.PRNGKey(0)\n_, tr = self.simulate(key, args)\nretval = tr.get_retval()\nreturn retval\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\nshape = kwargs.get(\"shape\", ())\nreturn Bottom(shape)\n@abc.abstractmethod\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n\"\"\"&gt; Given a `PRNGKey` and arguments, execute the generative function,\n        returning a new `PRNGKey` and a trace.\n        `simulate` can be informally thought of as forward sampling: given `key: PRNGKey` and arguments `args: Tuple`, the generative function should sample a choice map $c \\sim p(\\cdot; \\\\text{args})$, as well as any untraced randomness $r \\sim p(\\cdot; \\\\text{args}, c)$.\n        The implementation of `simulate` should then create a trace holding the choice map, as well as the score $\\log \\\\frac{p(c; \\\\text{args})}{q(r; \\\\text{args}, c)}$.\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            key: A new (deterministically evolved) `PRNGKey`.\n            tr: A trace capturing the data and inference data associated with the generative function invocation.\n        Examples:\n            Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            print(console.render(tr))\n            ```\n            Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                y = genjax.normal(x, 1.0) @ \"y\"\n                return y\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            print(console.render(tr))\n            ```\n        \"\"\"\ndef propose(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n\"\"\"&gt; Given a `PRNGKey` and arguments, execute the generative function,\n        returning a new `PRNGKey` and a tuple containing the return value from the generative function call, the score of the choice map assignment, and the choice map.\n        The default implementation just calls `simulate`, and then extracts the data from the `Trace` returned by `simulate`. Custom generative functions can overload the implementation for their own uses (e.g. if they don't have an associated `Trace` datatype, but can be uses as a proposal).\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            key: A new (deterministically evolved) `PRNGKey`.\n            tup: A tuple `(retval, w, chm)` where `retval` is the return value from the generative function invocation, `w` is the log joint density (or an importance weight estimate, in the case where there is untraced randomness), and `chm` is the choice map assignment from the invocation.\n        Examples:\n            Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, (r, w, chm) = genjax.normal.propose(key, (0.0, 1.0))\n            print(console.render(chm))\n            ```\n            Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                y = genjax.normal(x, 1.0) @ \"y\"\n                return y\n            key = jax.random.PRNGKey(314159)\n            key, (r, w, chm) = model.propose(key, ())\n            print(console.render(chm))\n            ```\n        \"\"\"\nkey, tr = self.simulate(key, args)\nchm = tr.get_choices()\nscore = tr.get_score()\nretval = tr.get_retval()\nreturn key, (retval, score, chm)\n@abc.abstractmethod\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, Trace]]:\n\"\"\"&gt; Given a `PRNGKey`, a choice map (constraints), and arguments,\n        execute the generative function, returning a new `PRNGKey`, a single-\n        sample importance weight estimate of the conditional density evaluated\n        at the non-constrained choices, and a trace whose choice map is\n        consistent with the constraints.\n        Arguments:\n            key: A `PRNGKey`.\n            args: Arguments to the generative function.\n        Returns:\n            key: A new (deterministically evolved) `PRNGKey`.\n            tup: A tuple `(w, tr)` where `w` is an importance weight estimate of the conditional density, and `tr` is a trace capturing the data and inference data associated with the generative function invocation.\n        \"\"\"\n@abc.abstractmethod\ndef update(\nself,\nkey: PRNGKey,\ntrace: Trace,\nnew: ChoiceMap,\ndiffs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, Trace, ChoiceMap]]:\npass\n@abc.abstractmethod\ndef assess(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\npass\ndef unzip(\nself,\nkey: PRNGKey,\nfixed: ChoiceMap,\n) -&gt; Tuple[\nPRNGKey,\nCallable[[ChoiceMap, Tuple], FloatArray],\nCallable[[ChoiceMap, Tuple], Any],\n]:\nkey, sub_key = jax.random.split(key)\ndef score(differentiable: Tuple, nondifferentiable: Tuple) -&gt; FloatArray:\nprovided, args = tree_zipper(differentiable, nondifferentiable)\nmerged = fixed.merge(provided)\n_, (_, score) = self.assess(sub_key, merged, args)\nreturn score\ndef retval(differentiable: Tuple, nondifferentiable: Tuple) -&gt; Any:\nprovided, args = tree_zipper(differentiable, nondifferentiable)\nmerged = fixed.merge(provided)\n_, (retval, _) = self.assess(sub_key, merged, args)\nreturn retval\nreturn key, score, retval\n# A higher-level gradient API - it relies upon `unzip`,\n# but provides convenient access to first-order gradients.\ndef choice_grad(self, key, trace, selection):\nfixed = selection.complement().filter(trace.strip())\nchm = selection.filter(trace.strip())\nkey, scorer, _ = self.unzip(key, fixed)\ngrad, nograd = tree_grad_split(\n(chm, trace.get_args()),\n)\nchoice_gradient_tree, _ = jax.grad(scorer)(grad, nograd)\nreturn key, choice_gradient_tree\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.simulate","title":"<code>simulate(key, args)</code>  <code>abstractmethod</code>","text":"<p>Given a <code>PRNGKey</code> and arguments, execute the generative function, returning a new <code>PRNGKey</code> and a trace.</p> <p><code>simulate</code> can be informally thought of as forward sampling: given <code>key: PRNGKey</code> and arguments <code>args: Tuple</code>, the generative function should sample a choice map \\(c \\sim p(\\cdot; \\text{args})\\), as well as any untraced randomness \\(r \\sim p(\\cdot; \\text{args}, c)\\).</p> <p>The implementation of <code>simulate</code> should then create a trace holding the choice map, as well as the score \\(\\log \\frac{p(c; \\text{args})}{q(r; \\text{args}, c)}\\).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>args</code> <code>Tuple</code> <p>Arguments to the generative function.</p> required <p>Returns:</p> Name Type Description <code>key</code> <code>PRNGKey</code> <p>A new (deterministically evolved) <code>PRNGKey</code>.</p> <code>tr</code> <code>Trace</code> <p>A trace capturing the data and inference data associated with the generative function invocation.</p> <p>Examples:</p> <p>Here's an example using a <code>genjax</code> distribution (<code>normal</code>). Distributions are generative functions, so they support the interface.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nprint(console.render(tr))\n</code></pre> <pre><code>DistributionTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 Normal\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u251c\u2500\u2500 (const) 0.0\n\u2502       \u2514\u2500\u2500 (const) 1.0\n\u251c\u2500\u2500 value\n\u2502   \u2514\u2500\u2500  f32[]\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> <p>Here's a slightly more complicated example using the <code>Builtin</code> generative function language. You can find more examples on the <code>Builtin</code> language page.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = genjax.normal(0.0, 1.0) @ \"x\"\ny = genjax.normal(x, 1.0) @ \"y\"\nreturn y\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nprint(console.render(tr))\n</code></pre> <pre><code>BuiltinTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502       \u2514\u2500\u2500 source\n\u2502           \u2514\u2500\u2500 &lt;function model&gt;\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u251c\u2500\u2500 retval\n\u2502   \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 choices\n\u2502   \u2514\u2500\u2500 Trie\n\u2502       \u251c\u2500\u2500 :x\n\u2502       \u2502   \u2514\u2500\u2500 DistributionTrace\n\u2502       \u2502       \u251c\u2500\u2500 gen_fn\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 Normal\n\u2502       \u2502       \u251c\u2500\u2500 args\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2502       \u251c\u2500\u2500 (const) 0.0\n\u2502       \u2502       \u2502       \u2514\u2500\u2500 (const) 1.0\n\u2502       \u2502       \u251c\u2500\u2500 value\n\u2502       \u2502       \u2502   \u2514\u2500\u2500  f32[]\n\u2502       \u2502       \u2514\u2500\u2500 score\n\u2502       \u2502           \u2514\u2500\u2500  f32[]\n\u2502       \u2514\u2500\u2500 :y\n\u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502               \u251c\u2500\u2500 gen_fn\n\u2502               \u2502   \u2514\u2500\u2500 Normal\n\u2502               \u251c\u2500\u2500 args\n\u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502               \u2502       \u251c\u2500\u2500  f32[]\n\u2502               \u2502       \u2514\u2500\u2500 (const) 1.0\n\u2502               \u251c\u2500\u2500 value\n\u2502               \u2502   \u2514\u2500\u2500  f32[]\n\u2502               \u2514\u2500\u2500 score\n\u2502                   \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 cache\n\u2502   \u2514\u2500\u2500 Trie\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef simulate(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n\"\"\"&gt; Given a `PRNGKey` and arguments, execute the generative function,\n    returning a new `PRNGKey` and a trace.\n    `simulate` can be informally thought of as forward sampling: given `key: PRNGKey` and arguments `args: Tuple`, the generative function should sample a choice map $c \\sim p(\\cdot; \\\\text{args})$, as well as any untraced randomness $r \\sim p(\\cdot; \\\\text{args}, c)$.\n    The implementation of `simulate` should then create a trace holding the choice map, as well as the score $\\log \\\\frac{p(c; \\\\text{args})}{q(r; \\\\text{args}, c)}$.\n    Arguments:\n        key: A `PRNGKey`.\n        args: Arguments to the generative function.\n    Returns:\n        key: A new (deterministically evolved) `PRNGKey`.\n        tr: A trace capturing the data and inference data associated with the generative function invocation.\n    Examples:\n        Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        print(console.render(tr))\n        ```\n        Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = genjax.normal(0.0, 1.0) @ \"x\"\n            y = genjax.normal(x, 1.0) @ \"y\"\n            return y\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        print(console.render(tr))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.propose","title":"<code>propose(key, args)</code>","text":"<p>Given a <code>PRNGKey</code> and arguments, execute the generative function, returning a new <code>PRNGKey</code> and a tuple containing the return value from the generative function call, the score of the choice map assignment, and the choice map.</p> <p>The default implementation just calls <code>simulate</code>, and then extracts the data from the <code>Trace</code> returned by <code>simulate</code>. Custom generative functions can overload the implementation for their own uses (e.g. if they don't have an associated <code>Trace</code> datatype, but can be uses as a proposal).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>args</code> <code>Tuple</code> <p>Arguments to the generative function.</p> required <p>Returns:</p> Name Type Description <code>key</code> <code>PRNGKey</code> <p>A new (deterministically evolved) <code>PRNGKey</code>.</p> <code>tup</code> <code>Trace</code> <p>A tuple <code>(retval, w, chm)</code> where <code>retval</code> is the return value from the generative function invocation, <code>w</code> is the log joint density (or an importance weight estimate, in the case where there is untraced randomness), and <code>chm</code> is the choice map assignment from the invocation.</p> <p>Examples:</p> <p>Here's an example using a <code>genjax</code> distribution (<code>normal</code>). Distributions are generative functions, so they support the interface.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, (r, w, chm) = genjax.normal.propose(key, (0.0, 1.0))\nprint(console.render(chm))\n</code></pre> <pre><code>ValueChoiceMap\n\u2514\u2500\u2500 value\n    \u2514\u2500\u2500  f32[]\n</code></pre> <p>Here's a slightly more complicated example using the <code>Builtin</code> generative function language. You can find more examples on the <code>Builtin</code> language page.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = genjax.normal(0.0, 1.0) @ \"x\"\ny = genjax.normal(x, 1.0) @ \"y\"\nreturn y\nkey = jax.random.PRNGKey(314159)\nkey, (r, w, chm) = model.propose(key, ())\nprint(console.render(chm))\n</code></pre> <pre><code>BuiltinChoiceMap\n\u2514\u2500\u2500 trie\n    \u2514\u2500\u2500 Trie\n        \u251c\u2500\u2500 :x\n        \u2502   \u2514\u2500\u2500 DistributionTrace\n        \u2502       \u251c\u2500\u2500 gen_fn\n        \u2502       \u2502   \u2514\u2500\u2500 Normal\n        \u2502       \u251c\u2500\u2500 args\n        \u2502       \u2502   \u2514\u2500\u2500 tuple\n        \u2502       \u2502       \u251c\u2500\u2500 (const) 0.0\n        \u2502       \u2502       \u2514\u2500\u2500 (const) 1.0\n        \u2502       \u251c\u2500\u2500 value\n        \u2502       \u2502   \u2514\u2500\u2500  f32[]\n        \u2502       \u2514\u2500\u2500 score\n        \u2502           \u2514\u2500\u2500  f32[]\n        \u2514\u2500\u2500 :y\n            \u2514\u2500\u2500 DistributionTrace\n                \u251c\u2500\u2500 gen_fn\n                \u2502   \u2514\u2500\u2500 Normal\n                \u251c\u2500\u2500 args\n                \u2502   \u2514\u2500\u2500 tuple\n                \u2502       \u251c\u2500\u2500  f32[]\n                \u2502       \u2514\u2500\u2500 (const) 1.0\n                \u251c\u2500\u2500 value\n                \u2502   \u2514\u2500\u2500  f32[]\n                \u2514\u2500\u2500 score\n                    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>def propose(\nself,\nkey: PRNGKey,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Trace]:\n\"\"\"&gt; Given a `PRNGKey` and arguments, execute the generative function,\n    returning a new `PRNGKey` and a tuple containing the return value from the generative function call, the score of the choice map assignment, and the choice map.\n    The default implementation just calls `simulate`, and then extracts the data from the `Trace` returned by `simulate`. Custom generative functions can overload the implementation for their own uses (e.g. if they don't have an associated `Trace` datatype, but can be uses as a proposal).\n    Arguments:\n        key: A `PRNGKey`.\n        args: Arguments to the generative function.\n    Returns:\n        key: A new (deterministically evolved) `PRNGKey`.\n        tup: A tuple `(retval, w, chm)` where `retval` is the return value from the generative function invocation, `w` is the log joint density (or an importance weight estimate, in the case where there is untraced randomness), and `chm` is the choice map assignment from the invocation.\n    Examples:\n        Here's an example using a `genjax` distribution (`normal`). Distributions are generative functions, so they support the interface.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, (r, w, chm) = genjax.normal.propose(key, (0.0, 1.0))\n        print(console.render(chm))\n        ```\n        Here's a slightly more complicated example using the `Builtin` generative function language. You can find more examples on the `Builtin` language page.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = genjax.normal(0.0, 1.0) @ \"x\"\n            y = genjax.normal(x, 1.0) @ \"y\"\n            return y\n        key = jax.random.PRNGKey(314159)\n        key, (r, w, chm) = model.propose(key, ())\n        print(console.render(chm))\n        ```\n    \"\"\"\nkey, tr = self.simulate(key, args)\nchm = tr.get_choices()\nscore = tr.get_score()\nretval = tr.get_retval()\nreturn key, (retval, score, chm)\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.importance","title":"<code>importance(key, chm, args)</code>  <code>abstractmethod</code>","text":"<p>Given a <code>PRNGKey</code>, a choice map (constraints), and arguments, execute the generative function, returning a new <code>PRNGKey</code>, a single- sample importance weight estimate of the conditional density evaluated at the non-constrained choices, and a trace whose choice map is consistent with the constraints.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>args</code> <code>Tuple</code> <p>Arguments to the generative function.</p> required <p>Returns:</p> Name Type Description <code>key</code> <code>PRNGKey</code> <p>A new (deterministically evolved) <code>PRNGKey</code>.</p> <code>tup</code> <code>Tuple[FloatArray, Trace]</code> <p>A tuple <code>(w, tr)</code> where <code>w</code> is an importance weight estimate of the conditional density, and <code>tr</code> is a trace capturing the data and inference data associated with the generative function invocation.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef importance(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, Trace]]:\n\"\"\"&gt; Given a `PRNGKey`, a choice map (constraints), and arguments,\n    execute the generative function, returning a new `PRNGKey`, a single-\n    sample importance weight estimate of the conditional density evaluated\n    at the non-constrained choices, and a trace whose choice map is\n    consistent with the constraints.\n    Arguments:\n        key: A `PRNGKey`.\n        args: Arguments to the generative function.\n    Returns:\n        key: A new (deterministically evolved) `PRNGKey`.\n        tup: A tuple `(w, tr)` where `w` is an importance weight estimate of the conditional density, and `tr` is a trace capturing the data and inference data associated with the generative function invocation.\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.assess","title":"<code>assess(key, chm, args)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef assess(\nself,\nkey: PRNGKey,\nchm: ChoiceMap,\nargs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\npass\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.GenerativeFunction.update","title":"<code>update(key, trace, new, diffs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef update(\nself,\nkey: PRNGKey,\ntrace: Trace,\nnew: ChoiceMap,\ndiffs: Tuple,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, Trace, ChoiceMap]]:\npass\n</code></pre>"},{"location":"genjax/library/core/generative.html#traces","title":"Traces","text":"<p>Traces are data structures which record (execution and inference) data about the invocation of generative functions.</p> <p>Traces are often specialized to a generative function language, to take advantage of data locality, and other representation optimizations.</p> <p>Traces support a set of accessor method interfaces designed to provide convenient manipulation when handling traces in inference algorithms.</p>"},{"location":"genjax/library/core/generative.html#genjax.core.Trace","title":"<code>genjax.core.Trace</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ChoiceMap</code>, <code>Tree</code></p> <p>Abstract base class for traces of generative functions.</p> <p>A <code>Trace</code> is a data structure used to represent sampled executions of generative functions.</p> <p>Traces track metadata associated with log probabilities of choices, as well as other data associated with the invocation of a generative function, including the arguments it was invoked with, its return value, and the identity of the generative function itself.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass Trace(ChoiceMap, Tree):\n\"\"\"&gt; Abstract base class for traces of generative functions.\n    A `Trace` is a data structure used to represent sampled executions\n    of generative functions.\n    Traces track metadata associated with log probabilities of choices,\n    as well as other data associated with the invocation of a generative\n    function, including the arguments it was invoked with, its return\n    value, and the identity of the generative function itself.\n    \"\"\"\n@abc.abstractmethod\ndef get_retval(self) -&gt; Any:\n\"\"\"Returns the return value from the generative function invocation\n        which created the `Trace`.\n        Examples:\n            Here's an example using `genjax.normal` (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            retval = tr.get_retval()\n            chm = tr.get_choices()\n            v = chm.get_leaf_value()\n            print(console.render((retval, v)))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_score(self) -&gt; FloatArray:\n\"\"\"\n        Return the joint log score of the `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            score = tr.get_score()\n            x_score = bernoulli.logpdf(tr[\"x\"], 0.3)\n            y_score = bernoulli.logpdf(tr[\"y\"], 0.3)\n            print(console.render((score, x_score + y_score)))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_args(self) -&gt; Tuple:\npass\n@abc.abstractmethod\ndef get_choices(self) -&gt; ChoiceMap:\n\"\"\"\n        Return a `ChoiceMap` representation of the set of traced random choices sampled during the execution of the generative function to produce the `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            chm = tr.get_choices()\n            print(console.render(chm))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef get_gen_fn(self) -&gt; \"GenerativeFunction\":\n\"\"\"Returns the generative function whose invocation created the\n        `Trace`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            gen_fn = tr.get_gen_fn()\n            print(console.render(gen_fn))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef project(self, selection: \"Selection\") -&gt; FloatArray:\n\"\"\"Given a `Selection`, return the total contribution to the joint log score of the addresses contained within the `Selection`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            selection = genjax.select(\"x\")\n            x_score = tr.project(selection)\n            x_score_t = genjax.bernoulli.logpdf(tr[\"x\"], 0.3)\n            print(console.render((x_score_t, x_score)))\n            ```\n        \"\"\"\ndef update(self, key, choices, argdiffs):\ngen_fn = self.get_gen_fn()\nreturn gen_fn.update(key, self, choices, argdiffs)\ndef has_subtree(self, addr) -&gt; BoolArray:\nchoices = self.get_choices()\nreturn choices.has_subtree(addr)\ndef get_subtree(self, addr) -&gt; ChoiceMap:\nchoices = self.get_choices()\nreturn choices.get_subtree(addr)\ndef get_subtrees_shallow(self):\nchoices = self.get_choices()\nreturn choices.get_subtrees_shallow()\ndef get_selection(self):\nreturn self.get_choices().get_selection()\ndef strip(self):\n\"\"\"Remove all `Trace` metadata, and return a choice map.\n        `ChoiceMap` instances produced by `tr.get_choices()` will preserve `Trace` instances. `strip` recursively calls `get_choices` to remove `Trace` instances.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            chm = tr.strip()\n            print(console.render(chm))\n            ```\n        \"\"\"\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self.get_choices(), is_leaf=_check)\ndef __getitem__(self, addr):\nchoices = self.get_choices()\nchoice = choices.get_subtree(addr)\nif isinstance(choice, BooleanMask):\nif is_concrete(choice.mask):\nif choice.mask:\nreturn choice.unmask()\nelse:\nreturn EmptyChoiceMap()\nelse:\nreturn choice\nelif isinstance(choice, Leaf):\nreturn choice.get_leaf_value()\nelse:\nreturn choice\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_gen_fn","title":"<code>get_gen_fn()</code>  <code>abstractmethod</code>","text":"<p>Returns the generative function whose invocation created the <code>Trace</code>.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\ngen_fn = tr.get_gen_fn()\nprint(console.render(gen_fn))\n</code></pre> <pre><code>Normal\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_gen_fn(self) -&gt; \"GenerativeFunction\":\n\"\"\"Returns the generative function whose invocation created the\n    `Trace`.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        gen_fn = tr.get_gen_fn()\n        print(console.render(gen_fn))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_retval","title":"<code>get_retval()</code>  <code>abstractmethod</code>","text":"<p>Returns the return value from the generative function invocation which created the <code>Trace</code>.</p> <p>Examples:</p> <p>Here's an example using <code>genjax.normal</code> (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nretval = tr.get_retval()\nchm = tr.get_choices()\nv = chm.get_leaf_value()\nprint(console.render((retval, v)))\n</code></pre> <pre><code>(Array(-0.10823099, dtype=float32), Array(-0.10823099, dtype=float32))\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_retval(self) -&gt; Any:\n\"\"\"Returns the return value from the generative function invocation\n    which created the `Trace`.\n    Examples:\n        Here's an example using `genjax.normal` (a distribution). For distributions, the return value is the same as the (only) value in the returned choice map.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        retval = tr.get_retval()\n        chm = tr.get_choices()\n        v = chm.get_leaf_value()\n        print(console.render((retval, v)))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_choices","title":"<code>get_choices()</code>  <code>abstractmethod</code>","text":"<p>Return a <code>ChoiceMap</code> representation of the set of traced random choices sampled during the execution of the generative function to produce the <code>Trace</code>.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nfrom genjax import bernoulli\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = bernoulli(0.3) @ \"x\"\ny = bernoulli(0.3) @ \"y\"\nreturn x\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nchm = tr.get_choices()\nprint(console.render(chm))\n</code></pre> <pre><code>BuiltinChoiceMap\n\u2514\u2500\u2500 trie\n    \u2514\u2500\u2500 Trie\n        \u251c\u2500\u2500 :x\n        \u2502   \u2514\u2500\u2500 DistributionTrace\n        \u2502       \u251c\u2500\u2500 gen_fn\n        \u2502       \u2502   \u2514\u2500\u2500 Bernoulli\n        \u2502       \u251c\u2500\u2500 args\n        \u2502       \u2502   \u2514\u2500\u2500 tuple\n        \u2502       \u2502       \u2514\u2500\u2500 (const) 0.3\n        \u2502       \u251c\u2500\u2500 value\n        \u2502       \u2502   \u2514\u2500\u2500  bool[]\n        \u2502       \u2514\u2500\u2500 score\n        \u2502           \u2514\u2500\u2500  f32[]\n        \u2514\u2500\u2500 :y\n            \u2514\u2500\u2500 DistributionTrace\n                \u251c\u2500\u2500 gen_fn\n                \u2502   \u2514\u2500\u2500 Bernoulli\n                \u251c\u2500\u2500 args\n                \u2502   \u2514\u2500\u2500 tuple\n                \u2502       \u2514\u2500\u2500 (const) 0.3\n                \u251c\u2500\u2500 value\n                \u2502   \u2514\u2500\u2500  bool[]\n                \u2514\u2500\u2500 score\n                    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_choices(self) -&gt; ChoiceMap:\n\"\"\"\n    Return a `ChoiceMap` representation of the set of traced random choices sampled during the execution of the generative function to produce the `Trace`.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        from genjax import bernoulli\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = bernoulli(0.3) @ \"x\"\n            y = bernoulli(0.3) @ \"y\"\n            return x\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        chm = tr.get_choices()\n        print(console.render(chm))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.get_score","title":"<code>get_score()</code>  <code>abstractmethod</code>","text":"<p>Return the joint log score of the <code>Trace</code>.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nfrom genjax import bernoulli\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = bernoulli(0.3) @ \"x\"\ny = bernoulli(0.3) @ \"y\"\nreturn x\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nscore = tr.get_score()\nx_score = bernoulli.logpdf(tr[\"x\"], 0.3)\ny_score = bernoulli.logpdf(tr[\"y\"], 0.3)\nprint(console.render((score, x_score + y_score)))\n</code></pre> <pre><code>(Array(-0.71334994, dtype=float32), Array(-0.71334994, dtype=float32))\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_score(self) -&gt; FloatArray:\n\"\"\"\n    Return the joint log score of the `Trace`.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        from genjax import bernoulli\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = bernoulli(0.3) @ \"x\"\n            y = bernoulli(0.3) @ \"y\"\n            return x\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        score = tr.get_score()\n        x_score = bernoulli.logpdf(tr[\"x\"], 0.3)\n        y_score = bernoulli.logpdf(tr[\"y\"], 0.3)\n        print(console.render((score, x_score + y_score)))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.strip","title":"<code>strip()</code>","text":"<p>Remove all <code>Trace</code> metadata, and return a choice map.</p> <p><code>ChoiceMap</code> instances produced by <code>tr.get_choices()</code> will preserve <code>Trace</code> instances. <code>strip</code> recursively calls <code>get_choices</code> to remove <code>Trace</code> instances.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nchm = tr.strip()\nprint(console.render(chm))\n</code></pre> <pre><code>ValueChoiceMap\n\u2514\u2500\u2500 value\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>def strip(self):\n\"\"\"Remove all `Trace` metadata, and return a choice map.\n    `ChoiceMap` instances produced by `tr.get_choices()` will preserve `Trace` instances. `strip` recursively calls `get_choices` to remove `Trace` instances.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        chm = tr.strip()\n        print(console.render(chm))\n        ```\n    \"\"\"\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self.get_choices(), is_leaf=_check)\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Trace.project","title":"<code>project(selection)</code>  <code>abstractmethod</code>","text":"<p>Given a <code>Selection</code>, return the total contribution to the joint log score of the addresses contained within the <code>Selection</code>.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nfrom genjax import bernoulli\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = bernoulli(0.3) @ \"x\"\ny = bernoulli(0.3) @ \"y\"\nreturn x\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nselection = genjax.select(\"x\")\nx_score = tr.project(selection)\nx_score_t = genjax.bernoulli.logpdf(tr[\"x\"], 0.3)\nprint(console.render((x_score_t, x_score)))\n</code></pre> <pre><code>(Array(-0.35667497, dtype=float32), Array(-0.35667497, dtype=float32))\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef project(self, selection: \"Selection\") -&gt; FloatArray:\n\"\"\"Given a `Selection`, return the total contribution to the joint log score of the addresses contained within the `Selection`.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        from genjax import bernoulli\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = bernoulli(0.3) @ \"x\"\n            y = bernoulli(0.3) @ \"y\"\n            return x\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        selection = genjax.select(\"x\")\n        x_score = tr.project(selection)\n        x_score_t = genjax.bernoulli.logpdf(tr[\"x\"], 0.3)\n        print(console.render((x_score_t, x_score)))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#choice-maps","title":"Choice maps","text":""},{"location":"genjax/library/core/generative.html#genjax.core.ChoiceMap","title":"<code>genjax.core.ChoiceMap</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Tree</code></p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass ChoiceMap(Tree):\n@abc.abstractmethod\ndef get_selection(self):\n\"\"\"Convert a `ChoiceMap` to a `Selection`.\"\"\"\ndef get_choices(self):\nreturn self\ndef strip(self):\ndef _check(v):\nreturn isinstance(v, Trace)\ndef _inner(v):\nif isinstance(v, Trace):\nreturn v.strip()\nelse:\nreturn v\nreturn jtu.tree_map(_inner, self, is_leaf=_check)\ndef __eq__(self, other):\nreturn self.flatten() == other.flatten()\ndef __getitem__(self, addr):\nchoice = self.get_subtree(addr)\nif isinstance(choice, Leaf):\nv = choice.get_leaf_value()\n# If the choice is a Leaf, it might participate in masking.\n# Here, we check if the value is masked.\n# Then, we either unwrap the mask - or return it,\n# depending on the concreteness of the mask value.\nif isinstance(v, BooleanMask):\nif is_concrete(v.mask):\nif v.mask:\nreturn v.unmask()\nelse:\nreturn EmptyChoiceMap()\nelse:\nreturn v\nelse:\nreturn v\nelse:\nreturn choice\ndef __setitem__(self, key, value):\nraise Exception(\n\"ChoiceMap of type {type(self)} does not implement __setitem__.\"\n)\ndef __add__(self, other):\nreturn self.merge(other)\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.ChoiceMap.get_selection","title":"<code>get_selection()</code>  <code>abstractmethod</code>","text":"<p>Convert a <code>ChoiceMap</code> to a <code>Selection</code>.</p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef get_selection(self):\n\"\"\"Convert a `ChoiceMap` to a `Selection`.\"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#selections","title":"Selections","text":""},{"location":"genjax/library/core/generative.html#genjax.core.Selection","title":"<code>genjax.core.Selection</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Tree</code></p> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@dataclasses.dataclass\nclass Selection(Tree):\n@abc.abstractmethod\ndef filter(self, chm: ChoiceMap) -&gt; ChoiceMap:\n\"\"\"Filter the addresses in a choice map, returning a new choice map.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            chm = tr.strip()\n            selection = genjax.select(\"x\")\n            filtered = selection.filter(chm)\n            print(console.render(filtered))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef complement(self) -&gt; \"Selection\":\n\"\"\"\n        Return a `Selection` which filters addresses to the complement set of the provided `Selection`.\n        Examples:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            console = genjax.pretty()\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n            key = jax.random.PRNGKey(314159)\n            key, tr = model.simulate(key, ())\n            chm = tr.strip()\n            selection = genjax.select(\"x\")\n            complement = selection.complement()\n            filtered = complement.filter(chm)\n            print(console.render(filtered))\n            ```\n        \"\"\"\ndef get_selection(self):\nreturn self\ndef __getitem__(self, addr):\nsubselection = self.get_subtree(addr)\nreturn subselection\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Selection.complement","title":"<code>complement()</code>  <code>abstractmethod</code>","text":"<p>Return a <code>Selection</code> which filters addresses to the complement set of the provided <code>Selection</code>.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nfrom genjax import bernoulli\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = bernoulli(0.3) @ \"x\"\ny = bernoulli(0.3) @ \"y\"\nreturn x\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nchm = tr.strip()\nselection = genjax.select(\"x\")\ncomplement = selection.complement()\nfiltered = complement.filter(chm)\nprint(console.render(filtered))\n</code></pre> <pre><code>BuiltinChoiceMap\n\u2514\u2500\u2500 trie\n    \u2514\u2500\u2500 Trie\n        \u2514\u2500\u2500 :y\n            \u2514\u2500\u2500 ValueChoiceMap\n                \u2514\u2500\u2500 value\n                    \u2514\u2500\u2500  bool[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef complement(self) -&gt; \"Selection\":\n\"\"\"\n    Return a `Selection` which filters addresses to the complement set of the provided `Selection`.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        from genjax import bernoulli\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = bernoulli(0.3) @ \"x\"\n            y = bernoulli(0.3) @ \"y\"\n            return x\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        chm = tr.strip()\n        selection = genjax.select(\"x\")\n        complement = selection.complement()\n        filtered = complement.filter(chm)\n        print(console.render(filtered))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/generative.html#genjax._src.core.datatypes.generative.Selection.filter","title":"<code>filter(chm)</code>  <code>abstractmethod</code>","text":"<p>Filter the addresses in a choice map, returning a new choice map.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nfrom genjax import bernoulli\nconsole = genjax.pretty()\n@genjax.gen\ndef model():\nx = bernoulli(0.3) @ \"x\"\ny = bernoulli(0.3) @ \"y\"\nreturn x\nkey = jax.random.PRNGKey(314159)\nkey, tr = model.simulate(key, ())\nchm = tr.strip()\nselection = genjax.select(\"x\")\nfiltered = selection.filter(chm)\nprint(console.render(filtered))\n</code></pre> <pre><code>BuiltinChoiceMap\n\u2514\u2500\u2500 trie\n    \u2514\u2500\u2500 Trie\n        \u2514\u2500\u2500 :x\n            \u2514\u2500\u2500 ValueChoiceMap\n                \u2514\u2500\u2500 value\n                    \u2514\u2500\u2500  bool[]\n</code></pre> Source code in <code>src/genjax/_src/core/datatypes/generative.py</code> <pre><code>@abc.abstractmethod\ndef filter(self, chm: ChoiceMap) -&gt; ChoiceMap:\n\"\"\"Filter the addresses in a choice map, returning a new choice map.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        from genjax import bernoulli\n        console = genjax.pretty()\n        @genjax.gen\n        def model():\n            x = bernoulli(0.3) @ \"x\"\n            y = bernoulli(0.3) @ \"y\"\n            return x\n        key = jax.random.PRNGKey(314159)\n        key, tr = model.simulate(key, ())\n        chm = tr.strip()\n        selection = genjax.select(\"x\")\n        filtered = selection.filter(chm)\n        print(console.render(filtered))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/core/interpreters.html","title":"Interpreters","text":"<p>JAX supports transformations of pure, numerical Python programs by staging out interpreters which evaluate <code>Jaxpr</code> representations of programs.</p> <p>The <code>Core</code> module features interpreter infrastructure, and common transforms designed to facilitate certain types of transformations.</p>"},{"location":"genjax/library/core/interpreters.html#contextual-interpreter","title":"Contextual interpreter","text":"<p>A common type of interpreter involves overloading desired primitives with context-specific behavior by inheriting from <code>Trace</code> and define the correct methods to process the primitives.</p> <p>In this module, we provide an interpreter which mixes initial style (e.g. the Python program is immediately staged, and then an interpreter walks the <code>Jaxpr</code> representation) with custom <code>Trace</code> and <code>Tracer</code> overloads. </p> <p>This pattern supports a wide range of program transformations, and allows parametrization over the inner interpreter (e.g. forward evaluation, or CPS).</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context","title":"<code>genjax._src.core.interpreters.context</code>","text":"<p>This module contains a transformation infrastructure based on interpreters with stateful contexts and custom primitive handling lookups.</p>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context.ContextualTracer","title":"<code>ContextualTracer</code>","text":"<p>         Bases: <code>jc.Tracer</code></p> <p>A <code>ContextualTracer</code> encapsulates a single value.</p> Source code in <code>src/genjax/_src/core/interpreters/context.py</code> <pre><code>class ContextualTracer(jc.Tracer):\n\"\"\"A `ContextualTracer` encapsulates a single value.\"\"\"\ndef __init__(self, trace: \"ContextualTrace\", val: Value):\nself._trace = trace\nself.val = val\n@property\ndef aval(self):\nreturn jc.raise_to_shaped(jc.get_aval(self.val))\ndef full_lower(self):\nreturn self\n</code></pre>"},{"location":"genjax/library/core/interpreters.html#genjax._src.core.interpreters.context.ContextualTrace","title":"<code>ContextualTrace</code>","text":"<p>         Bases: <code>jc.Trace</code></p> <p>An evaluating trace that dispatches to a dynamic context.</p> Source code in <code>src/genjax/_src/core/interpreters/context.py</code> <pre><code>class ContextualTrace(jc.Trace):\n\"\"\"An evaluating trace that dispatches to a dynamic context.\"\"\"\ndef pure(self, val: Value) -&gt; ContextualTracer:\nreturn ContextualTracer(self, val)\ndef sublift(self, tracer: ContextualTracer) -&gt; ContextualTracer:\nreturn self.pure(tracer.val)\ndef lift(self, val: Value) -&gt; ContextualTracer:\nreturn self.pure(val)\ndef process_primitive(\nself,\nprimitive: jc.Primitive,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n) -&gt; Union[ContextualTracer, List[ContextualTracer]]:\ncontext = staging.get_dynamic_context(self)\ncustom_rule = context.get_custom_rule(primitive)\nif custom_rule:\nreturn custom_rule(self, *tracers, **params)\nreturn self.default_process_primitive(primitive, tracers, params)\ndef default_process_primitive(\nself,\nprimitive: jc.Primitive,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n) -&gt; Union[ContextualTracer, List[ContextualTracer]]:\ncontext = staging.get_dynamic_context(self)\nvals = [v.val for v in tracers]\nif context.can_process(primitive):\noutvals = context.process_primitive(primitive, *vals, **params)\nreturn jax_util.safe_map(self.pure, outvals)\noutvals = primitive.bind(*vals, **params)\nif not primitive.multiple_results:\noutvals = [outvals]\nout_tracers = jax_util.safe_map(self.full_raise, outvals)\nif primitive.multiple_results:\nreturn out_tracers\nreturn out_tracers[0]\ndef process_call(\nself,\ncall_primitive: jc.Primitive,\nf: Any,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_higher_order_primitive(\nself, call_primitive, f, tracers, params, False\n)\ndef post_process_call(self, call_primitive, out_tracers, params):\nvals = tuple(t.val for t in out_tracers)\nmaster = self.main\ndef todo(x):\ntrace = ContextualTrace(master, jc.cur_sublevel())\nreturn jax_util.safe_map(functools.partial(ContextualTracer, trace), x)\nreturn vals, todo\ndef process_map(\nself,\ncall_primitive: jc.Primitive,\nf: Any,\ntracers: List[ContextualTracer],\nparams: Dict[str, Any],\n):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_higher_order_primitive(\nself, call_primitive, f, tracers, params, True\n)\npost_process_map = post_process_call\ndef process_custom_jvp_call(self, primitive, fun, jvp, tracers, *, symbolic_zeros):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_custom_jvp_call(\nself, primitive, fun, jvp, tracers, symbolic_zeros=symbolic_zeros\n)\ndef post_process_custom_jvp_call(self, out_tracers, jvp_was_run):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_jvp_call(self, out_tracers, jvp_was_run)\ndef process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers, out_trees):\ncontext = staging.get_dynamic_context(self)\nreturn context.process_custom_vjp_call(\nself, primitive, fun, fwd, bwd, tracers, out_trees\n)\ndef post_process_custom_vjp_call(self, out_tracers, params):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_vjp_call(self, out_tracers, params)\ndef post_process_custom_vjp_call_fwd(self, out_tracers, out_trees):\ncontext = staging.get_dynamic_context(self)\nreturn context.post_process_custom_vjp_call_fwd(self, out_tracers, out_trees)\n</code></pre>"},{"location":"genjax/library/extensions/index.html","title":"Extension modules","text":"<p>Because the set of generative function types is extensible, languages which extend Gen's interfaces to new types of generative computation are a natural part of Gen's ecosystem.</p>"},{"location":"genjax/library/extensions/index.html#lazy-loading","title":"Lazy loading","text":"<p>Users who wish to implement extension modules whose functionality depends on external 3rd party packages have a few patterns at their disposal. A user can simply build off <code>genjax</code> in their own repo, implementing their own generative functions, and inference, etc - while relying on 3rd party dependencies as well. This is likely the most common pattern, and should be a happy path for extension.</p> <p>Extension modules which are considered useful and worthy of trunk support (to be bundled and tested with the <code>genjax</code> system) can utilize a lazy loading system:</p>"},{"location":"genjax/library/extensions/index.html#genjax.extras.LazyLoader","title":"<code>genjax.extras.LazyLoader</code>","text":"<p>         Bases: <code>types.ModuleType</code></p> <p>A lazy loading system which allows extension modules to optionally depend on 3rd party dependencies which may be too heavyweight to include as required dependencies for <code>genjax</code> proper.</p> <p>Examples:</p> <p>To utilize the system, the <code>LazyLoader</code> expects that you provide a local name for the module, globals, and the source module. Here's example usage for an extension module utilizing <code>tinygp</code> - we give the lazy loaded module the name <code>tinygp</code>, and tell the loader that the module path is <code>genjax._src.extras.tinygp</code>:</p> <pre><code># tinygp provides Gaussian process model ingredients.\ntinygp = LazyLoader(\n\"tinygp\",\nglobals(),\n\"genjax._src.extras.tinygp\",\n)\n</code></pre> <p>The <code>tinygp</code> and <code>blackjax</code> extension modules rely on this system to implement functionality, while optionally depending on the presence of <code>tinygp</code> and <code>blackjax</code> (both 3rd party dependencies) for usage.</p> SourceResult <pre><code>import jax\nimport genjax\nimport tinygp.kernels as kernels\nconsole = genjax.pretty()\n# Extension module\ntinygp = genjax.extras.tinygp\nkernel_scaled = 4.5 * kernels.ExpSquared(scale=1.5)\nmodel = tinygp.GaussianProcess(kernel_scaled)\nprint(console.render(model))\n</code></pre> <pre><code>GaussianProcess\n\u2514\u2500\u2500 kernel\n    \u2514\u2500\u2500 Product\n        \u251c\u2500\u2500 kernel1\n        \u2502   \u2514\u2500\u2500 Constant\n        \u2502       \u2514\u2500\u2500 value\n        \u2502           \u2514\u2500\u2500 (const) 4.5\n        \u2514\u2500\u2500 kernel2\n            \u2514\u2500\u2500 ExpSquared\n                \u251c\u2500\u2500 scale\n                \u2502   \u2514\u2500\u2500 (const) 1.5\n                \u2514\u2500\u2500 distance\n                    \u2514\u2500\u2500 L2Distance\n</code></pre> Source code in <code>src/genjax/_src/extras/__init__.py</code> <pre><code>class LazyLoader(types.ModuleType):\n\"\"\"&gt; A lazy loading system which allows extension modules to optionally\n    depend on 3rd party dependencies which may be too heavyweight to include as\n    required dependencies for `genjax` proper.\n    Examples:\n        To utilize the system, the `LazyLoader` expects that you provide a local name for the module, globals, and the source module. Here's example usage for an extension module utilizing `tinygp` - we give the lazy loaded module the name `tinygp`, and tell the loader that the module path is `genjax._src.extras.tinygp`:\n        ```python\n        # tinygp provides Gaussian process model ingredients.\n        tinygp = LazyLoader(\n            \"tinygp\",\n            globals(),\n            \"genjax._src.extras.tinygp\",\n        )\n        ```\n        The `tinygp` and `blackjax` extension modules rely on this system to implement functionality, while optionally depending on the presence of `tinygp` and `blackjax` (both 3rd party dependencies) for usage.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        import tinygp.kernels as kernels\n        console = genjax.pretty()\n        # Extension module\n        tinygp = genjax.extras.tinygp\n        kernel_scaled = 4.5 * kernels.ExpSquared(scale=1.5)\n        model = tinygp.GaussianProcess(kernel_scaled)\n        print(console.render(model))\n        ```\n    \"\"\"\ndef __init__(self, local_name, parent_module_globals, name):\nself._local_name = local_name\nself._parent_module_globals = parent_module_globals\nsuper(LazyLoader, self).__init__(name)\ndef _load(self):\ntry:\nmodule = importlib.import_module(self.__name__)\nself._parent_module_globals[self._local_name] = module\nself.__dict__.update(module.__dict__)\nreturn module\nexcept ModuleNotFoundError as e:\ne.add_note(\nf\"(GenJAX) Attempted to load {self._local_name} extension but failed, is it installed in your environment?\"\n)\nraise e\ndef __getattr__(self, item):\nmodule = self._load()\nreturn getattr(module, item)\ndef __dir__(self):\nmodule = self._load()\nreturn dir(module)\n</code></pre>"},{"location":"genjax/library/extensions/index.html#genjax._src.extras.LazyLoader._load","title":"<code>_load()</code>","text":"Source code in <code>src/genjax/_src/extras/__init__.py</code> <pre><code>def _load(self):\ntry:\nmodule = importlib.import_module(self.__name__)\nself._parent_module_globals[self._local_name] = module\nself.__dict__.update(module.__dict__)\nreturn module\nexcept ModuleNotFoundError as e:\ne.add_note(\nf\"(GenJAX) Attempted to load {self._local_name} extension but failed, is it installed in your environment?\"\n)\nraise e\n</code></pre>"},{"location":"genjax/library/extensions/index.html#genjax._src.extras.LazyLoader.__getattr__","title":"<code>__getattr__(item)</code>","text":"Source code in <code>src/genjax/_src/extras/__init__.py</code> <pre><code>def __getattr__(self, item):\nmodule = self._load()\nreturn getattr(module, item)\n</code></pre>"},{"location":"genjax/library/extensions/index.html#genjax._src.extras.LazyLoader.__dir__","title":"<code>__dir__()</code>","text":"Source code in <code>src/genjax/_src/extras/__init__.py</code> <pre><code>def __dir__(self):\nmodule = self._load()\nreturn dir(module)\n</code></pre>"},{"location":"genjax/library/extensions/index.html#currently-supported-modules","title":"Currently supported modules","text":"<p>Here, we document two current modules:</p> <ul> <li>An extension module which extends the generative function interface to Gaussian processes using functionality from <code>tinygp</code>.</li> <li>An extension module for inference which provides a compatibility layer between generative functions and <code>blackjax</code> for state-of-the-art Hamiltonian Monte Carlo (HMC) and No-U-Turn sampling (NUTS) inference.</li> </ul>"},{"location":"genjax/library/extensions/blackjax.html","title":"HMC/NUTS from <code>blackjax</code>","text":""},{"location":"genjax/library/extensions/tinygp.html","title":"Gaussian processes from <code>tinygp</code>","text":""},{"location":"genjax/library/generative_functions/index.html","title":"Generative function languages","text":""},{"location":"genjax/library/generative_functions/index.html#genjax._src.generative_functions","title":"<code>genjax._src.generative_functions</code>","text":"<p>This module contains several standard generative function classes useful for structuring probabilistic programs.</p> <ul> <li>The <code>distributions</code> module exports standard distributions from several sources, including SciPy (<code>scipy</code>), TensorFlow Probability Distributions (<code>tfd</code>), and custom distributions.<ul> <li>The <code>distributions</code> module also contains a small <code>oryx</code>-like language called <code>coryx</code> which implements the generative function interface for programs with inverse log determinant Jacobian (ildj) compatible return value functions of distribution random choices.</li> <li>The <code>distributions</code> module also contains an implementation of <code>gensp</code>, a research language for probabilistic programming with estimated densities.</li> </ul> </li> <li>The <code>builtin</code> module contains a function-like language for defining generative functions from programs.</li> <li>The <code>combinators</code> module contains combinators which support transforming generative functions into new ones with structured control flow patterns of computation.</li> </ul>"},{"location":"genjax/library/generative_functions/builtin.html","title":"Builtin language","text":"<p>This module provides a function-like modeling language. The generative function interfaces are implemented for objects in this language using transformations by JAX interpreters.</p> <p>The language also exposes a set of JAX primitives which allow hierarchical construction of generative programs. These programs can utilize other generative functions inside of a new JAX primitive (<code>trace</code>) to create hierarchical patterns of generative computation.</p>"},{"location":"genjax/library/generative_functions/builtin.html#usage","title":"Usage","text":"<p>The <code>Builtin</code> language is a common foundation for constructing models. It exposes a DSL based on JAX primitives and transformations which allows the programmer to construct generative functions out of Python functions. </p> <p>Below, we illustrate a simple example:</p> <pre><code>from genjax import beta \nfrom genjax import bernoulli \nfrom genjax import uniform \nfrom genjax import gen\n@genjax.gen\ndef beta_bernoulli_process(u):\np = beta(0, u) @ \"p\"\nv = bernoulli(p) @ \"v\"\nreturn v\n@genjax.gen\ndef joint():\nu = uniform() @ \"u\"\nv = beta_bernoulli_process(u) @ \"bbp\"\nreturn v\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#language-primitives","title":"Language primitives","text":"<p>The builtin language exposes custom primitives, which are handled by JAX interpreters to support the semantics of the generative function interface.</p>"},{"location":"genjax/library/generative_functions/builtin.html#trace","title":"<code>trace</code>","text":"<p>The <code>trace</code> primitive provides access to the ability to invoke another generative function as a callee. Returning to our example above:</p> SourceResult <pre><code>import genjax\nfrom genjax import beta \nfrom genjax import bernoulli \nfrom genjax import gen\n@gen\ndef beta_bernoulli_process(u):\n# Invoking `trace` can be sweetened, or unsweetened.\np = genjax.trace(\"p\", beta)(0, u) # not sweet\nv = bernoulli(p) @ \"v\" # sweet\nreturn v\n</code></pre> <p>Now, programs written in the DSL which utilize <code>trace</code> have generative function interface method implementations which store callee choice data in the trace:</p> SourceResult <pre><code>import jax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = beta_bernoulli_process.simulate(key, (2, ))\nprint(console.render(tr))\n</code></pre> <pre><code>BuiltinTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502       \u2514\u2500\u2500 source\n\u2502           \u2514\u2500\u2500 &lt;function beta_bernoulli_process&gt;\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2514\u2500\u2500 (const) 2\n\u251c\u2500\u2500 retval\n\u2502   \u2514\u2500\u2500  bool[]\n\u251c\u2500\u2500 choices\n\u2502   \u2514\u2500\u2500 Trie\n\u2502       \u251c\u2500\u2500 :p\n\u2502       \u2502   \u2514\u2500\u2500 DistributionTrace\n\u2502       \u2502       \u251c\u2500\u2500 gen_fn\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 Beta\n\u2502       \u2502       \u251c\u2500\u2500 args\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2502       \u251c\u2500\u2500 (const) 0\n\u2502       \u2502       \u2502       \u2514\u2500\u2500 (const) 2\n\u2502       \u2502       \u251c\u2500\u2500 value\n\u2502       \u2502       \u2502   \u2514\u2500\u2500  f32[]\n\u2502       \u2502       \u2514\u2500\u2500 score\n\u2502       \u2502           \u2514\u2500\u2500  f32[]\n\u2502       \u2514\u2500\u2500 :v\n\u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502               \u251c\u2500\u2500 gen_fn\n\u2502               \u2502   \u2514\u2500\u2500 Bernoulli\n\u2502               \u251c\u2500\u2500 args\n\u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502               \u2502       \u2514\u2500\u2500  f32[]\n\u2502               \u251c\u2500\u2500 value\n\u2502               \u2502   \u2514\u2500\u2500  bool[]\n\u2502               \u2514\u2500\u2500 score\n\u2502                   \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 cache\n\u2502   \u2514\u2500\u2500 Trie\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> <p>Notice how the rendered result <code>Trace</code> has addresses in its choice trie for <code>\"p\"</code> and <code>\"v\"</code> - corresponding to the invocation of the beta and Bernoulli distribution generative functions.</p> <p>The <code>trace</code> primitive is a critical element of structuring hierarchical generative computation in the builtin language.</p>"},{"location":"genjax/library/generative_functions/builtin.html#cache","title":"<code>cache</code>","text":"<p>The <code>cache</code> primitive is designed to expose a space vs. time trade-off for incremental computation in Gen's <code>update</code> interface.</p>"},{"location":"genjax/library/generative_functions/builtin.html#generative-datatypes","title":"Generative datatypes","text":"<p>The builtin language implements a trie-like trace, choice map, and selection.</p>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.BuiltinTrace","title":"<code>genjax.generative_functions.builtin.BuiltinTrace</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Trace</code></p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_datatypes.py</code> <pre><code>@dataclass\nclass BuiltinTrace(Trace):\ngen_fn: GenerativeFunction\nargs: Tuple\nretval: Any\nchoices: Trie\ncache: Trie\nscore: FloatArray\ndef flatten(self):\nreturn (\nself.gen_fn,\nself.args,\nself.retval,\nself.choices,\nself.cache,\nself.score,\n), ()\ndef get_gen_fn(self):\nreturn self.gen_fn\ndef get_choices(self):\nreturn BuiltinChoiceMap(self.choices)\ndef get_retval(self):\nreturn self.retval\ndef get_score(self):\nreturn self.score\ndef get_args(self):\nreturn self.args\ndef project(self, selection: Selection):\nweight = 0.0\nfor (k, v) in self.choices.get_subtrees_shallow():\nif selection.has_subtree(k):\nweight += v.project(selection[k])\nreturn weight\ndef has_cached_value(self, addr):\nreturn self.cache.has_subtree(addr)\ndef get_cached_value(self, addr):\nreturn self.cache.get_subtree(addr)\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.BuiltinChoiceMap","title":"<code>genjax.generative_functions.builtin.BuiltinChoiceMap</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_datatypes.py</code> <pre><code>@dataclass\nclass BuiltinChoiceMap(ChoiceMap):\ntrie: Trie\ndef flatten(self):\nreturn (self.trie,), ()\n@typecheck\n@classmethod\ndef new(cls, constraints: Dict):\nassert isinstance(constraints, Dict)\ntrie = Trie.new()\nfor (k, v) in constraints.items():\nv = (\nValueChoiceMap(v)\nif not isinstance(v, ChoiceMap) and not isinstance(v, Trace)\nelse v\n)\ntrie.trie_insert(k, v)\nreturn BuiltinChoiceMap(trie)\ndef has_subtree(self, addr):\nreturn self.trie.has_subtree(addr)\ndef get_subtree(self, addr):\nvalue = self.trie.get_subtree(addr)\nif value is None:\nreturn EmptyChoiceMap()\nelse:\nreturn value.get_choices()\ndef get_subtrees_shallow(self):\nreturn map(\nlambda v: (v[0], v[1].get_choices()),\nself.trie.get_subtrees_shallow(),\n)\ndef get_selection(self):\ntrie = Trie.new()\nfor (k, v) in self.get_subtrees_shallow():\ntrie[k] = v.get_selection()\nreturn BuiltinSelection(trie)\n# TODO: test this.\ndef merge(self, other: \"BuiltinChoiceMap\"):\nassert isinstance(other, BuiltinChoiceMap)\nnew_inner = self.trie.merge(other.trie)\nreturn BuiltinChoiceMap(new_inner)\ndef __setitem__(self, k, v):\nv = (\nValueChoiceMap(v)\nif not isinstance(v, ChoiceMap) and not isinstance(v, Trace)\nelse v\n)\nself.trie.trie_insert(k, v)\ndef __hash__(self):\nreturn hash(self.trie)\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.BuiltinSelection","title":"<code>genjax.generative_functions.builtin.BuiltinSelection</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Selection</code></p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_datatypes.py</code> <pre><code>@dataclass\nclass BuiltinSelection(Selection):\ntrie: Trie\ndef flatten(self):\nreturn (self.trie,), ()\n@typecheck\n@classmethod\ndef new(cls, *addrs):\ntrie = Trie.new()\nfor addr in addrs:\ntrie[addr] = AllSelection()\nreturn BuiltinSelection(trie)\n@typecheck\n@classmethod\ndef with_selections(cls, selections: Dict):\nassert isinstance(selections, Dict)\ntrie = Trie.new()\nfor (k, v) in selections.items():\nassert isinstance(v, Selection)\ntrie.trie_insert(k, v)\nreturn BuiltinSelection(trie)\ndef filter(self, tree):\ndef _inner(k, v):\nsub = self.trie[k]\nif sub is None:\nsub = NoneSelection()\n# Handles hierarchical in Trie.\nelif isinstance(sub, Trie):\nsub = BuiltinSelection(sub)\nunder = sub.filter(v)\nreturn k, under\ntrie = Trie.new()\niter = tree.get_subtrees_shallow()\nfor (k, v) in map(lambda args: _inner(*args), iter):\nif not isinstance(v, EmptyChoiceMap):\ntrie[k] = v\nif isinstance(tree, TraceType):\nreturn type(tree)(trie, tree.get_rettype())\nelse:\nreturn BuiltinChoiceMap(trie)\ndef complement(self):\nreturn BuiltinComplementSelection(self.trie)\ndef has_subtree(self, addr):\nreturn self.trie.has_subtree(addr)\ndef get_subtree(self, addr):\nvalue = self.trie.get_subtree(addr)\nif value is None:\nreturn NoneSelection()\nelse:\nreturn value\ndef get_subtrees_shallow(self):\nreturn self.trie.get_subtrees_shallow()\n</code></pre>"},{"location":"genjax/library/generative_functions/builtin.html#genjax.generative_functions.builtin.BuiltinComplementSelection","title":"<code>genjax.generative_functions.builtin.BuiltinComplementSelection</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Selection</code></p> Source code in <code>src/genjax/_src/generative_functions/builtin/builtin_datatypes.py</code> <pre><code>@dataclass\nclass BuiltinComplementSelection(Selection):\ntrie: Trie\ndef flatten(self):\nreturn (self.selection,), ()\ndef filter(self, chm):\ndef _inner(k, v):\nsub = self.trie[k]\nif sub is None:\nsub = NoneSelection()\n# Handles hierarchical in Trie.\nelif isinstance(sub, Trie):\nsub = BuiltinSelection(sub)\nunder = sub.complement().filter(v)\nreturn k, under\ntrie = Trie.new()\niter = chm.get_subtrees_shallow()\nfor (k, v) in map(lambda args: _inner(*args), iter):\nif not isinstance(v, EmptyChoiceMap):\ntrie[k] = v\nif isinstance(chm, TraceType):\nreturn type(chm)(trie, chm.get_rettype())\nelse:\nreturn BuiltinChoiceMap(trie)\ndef complement(self):\nreturn BuiltinSelection(self.trie)\ndef has_subtree(self, addr):\nreturn self.trie.has_subtree(addr)\ndef get_subtree(self, addr):\nvalue = self.trie.get_subtree(addr)\nif value is None:\nreturn NoneSelection()\nelse:\nreturn value\ndef get_subtrees_shallow(self):\nreturn self.trie.get_subtrees_shallow()\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/index.html","title":"Generative function combinators","text":""},{"location":"genjax/library/generative_functions/combinators/index.html#genjax._src.generative_functions.combinators","title":"<code>genjax._src.generative_functions.combinators</code>","text":"<p>The <code>combinators</code> module exposes generative function combinators, generative functions which accept other generative functions as configuration arguments, and implement structured patterns of control flow (as well as other types of modifications) as their generative function interface implementations.</p> <p>GenJAX features several standard combinators:</p> <ul> <li><code>UnfoldCombinator</code> - which exposes a scan-like pattern for generative computation in a state space pattern via implementations utilizing <code>jax.lax.scan</code>.</li> <li><code>MapCombinator</code> - which exposes generative vectorization over input arguments, whose implementation utilizes <code>jax.vmap</code>.</li> <li><code>SwitchCombinator</code> - which exposes stochastic branching patterns utilizing <code>jax.lax.switch</code>.</li> </ul>"},{"location":"genjax/library/generative_functions/combinators/map.html","title":"Map combinator","text":"<p>GenJAX's <code>MapCombinator</code> is a combinator which exposes vectorization to the input arguments of generative functions.</p>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax.generative_functions.combinators.MapCombinator","title":"<code>genjax.generative_functions.combinators.MapCombinator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>GenerativeFunction</code></p> <p><code>MapCombinator</code> accepts a generative function as input and provides <code>vmap</code>-based implementations of the generative function interface methods.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef add_normal_noise(x):\nnoise1 = genjax.normal(0.0, 1.0) @ \"noise1\"\nnoise2 = genjax.normal(0.0, 1.0) @ \"noise2\"\nreturn x + noise1 + noise2\n# Creating a `MapCombinator` via the preferred `new` class method.\nmapped = genjax.MapCombinator.new(add_normal_noise, in_axes=(0,))\nkey = jax.random.PRNGKey(314159)\narr = jnp.ones(100)\nkey, tr = jax.jit(genjax.simulate(mapped))(key, (arr, ))\nprint(console.render(tr))\n</code></pre> <pre><code>VectorTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 MapCombinator\n\u2502       \u251c\u2500\u2500 in_axes\n\u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2514\u2500\u2500 (const) 0\n\u2502       \u251c\u2500\u2500 repeats\n\u2502       \u2502   \u2514\u2500\u2500 (const) None\n\u2502       \u2514\u2500\u2500 kernel\n\u2502           \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502               \u2514\u2500\u2500 source\n\u2502                   \u2514\u2500\u2500 &lt;function add_normal_noise&gt;\n\u251c\u2500\u2500 indices\n\u2502   \u2514\u2500\u2500  i32[100]\n\u251c\u2500\u2500 inner\n\u2502   \u2514\u2500\u2500 BuiltinTrace\n\u2502       \u251c\u2500\u2500 gen_fn\n\u2502       \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502       \u2502       \u2514\u2500\u2500 source\n\u2502       \u2502           \u2514\u2500\u2500 &lt;function add_normal_noise&gt;\n\u2502       \u251c\u2500\u2500 args\n\u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2514\u2500\u2500  f32[100]\n\u2502       \u251c\u2500\u2500 retval\n\u2502       \u2502   \u2514\u2500\u2500  f32[100]\n\u2502       \u251c\u2500\u2500 choices\n\u2502       \u2502   \u2514\u2500\u2500 Trie\n\u2502       \u2502       \u251c\u2500\u2500 :noise1\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 DistributionTrace\n\u2502       \u2502       \u2502       \u251c\u2500\u2500 gen_fn\n\u2502       \u2502       \u2502       \u2502   \u2514\u2500\u2500 Normal\n\u2502       \u2502       \u2502       \u251c\u2500\u2500 args\n\u2502       \u2502       \u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2502       \u2502       \u251c\u2500\u2500  f32[100]\n\u2502       \u2502       \u2502       \u2502       \u2514\u2500\u2500  f32[100]\n\u2502       \u2502       \u2502       \u251c\u2500\u2500 value\n\u2502       \u2502       \u2502       \u2502   \u2514\u2500\u2500  f32[100]\n\u2502       \u2502       \u2502       \u2514\u2500\u2500 score\n\u2502       \u2502       \u2502           \u2514\u2500\u2500  f32[100]\n\u2502       \u2502       \u2514\u2500\u2500 :noise2\n\u2502       \u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502       \u2502               \u251c\u2500\u2500 gen_fn\n\u2502       \u2502               \u2502   \u2514\u2500\u2500 Normal\n\u2502       \u2502               \u251c\u2500\u2500 args\n\u2502       \u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502               \u2502       \u251c\u2500\u2500  f32[100]\n\u2502       \u2502               \u2502       \u2514\u2500\u2500  f32[100]\n\u2502       \u2502               \u251c\u2500\u2500 value\n\u2502       \u2502               \u2502   \u2514\u2500\u2500  f32[100]\n\u2502       \u2502               \u2514\u2500\u2500 score\n\u2502       \u2502                   \u2514\u2500\u2500  f32[100]\n\u2502       \u251c\u2500\u2500 cache\n\u2502       \u2502   \u2514\u2500\u2500 Trie\n\u2502       \u2514\u2500\u2500 score\n\u2502           \u2514\u2500\u2500  f32[100]\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2514\u2500\u2500  f32[100]\n\u251c\u2500\u2500 retval\n\u2502   \u2514\u2500\u2500  f32[100]\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/map_combinator.py</code> <pre><code>@dataclass\nclass MapCombinator(GenerativeFunction):\n\"\"\"&gt; `MapCombinator` accepts a generative function as input and provides\n    `vmap`-based implementations of the generative function interface methods.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import jax.numpy as jnp\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def add_normal_noise(x):\n            noise1 = genjax.normal(0.0, 1.0) @ \"noise1\"\n            noise2 = genjax.normal(0.0, 1.0) @ \"noise2\"\n            return x + noise1 + noise2\n        # Creating a `MapCombinator` via the preferred `new` class method.\n        mapped = genjax.MapCombinator.new(add_normal_noise, in_axes=(0,))\n        key = jax.random.PRNGKey(314159)\n        arr = jnp.ones(100)\n        key, tr = jax.jit(genjax.simulate(mapped))(key, (arr, ))\n        print(console.render(tr))\n        ```\n    \"\"\"\nin_axes: Tuple\nrepeats: Union[None, IntArray]\nkernel: GenerativeFunction\ndef flatten(self):\nreturn (self.kernel,), (self.in_axes, self.repeats)\n# This overloads the call functionality for this generative function\n# and allows usage of shorthand notation in the builtin DSL.\ndef __call__(self, *args, **kwargs) -&gt; DeferredGenerativeFunctionCall:\nreturn DeferredGenerativeFunctionCall.new(self, args, kwargs)\n@typecheck\n@classmethod\ndef new(\ncls,\nkernel: GenerativeFunction,\nin_axes: Union[None, Tuple] = None,\nrepeats: Union[None, IntArray] = None,\n) -&gt; \"MapCombinator\":\n\"\"\"The preferred constructor for `MapCombinator` generative function\n        instances. The shorthand symbol is `Map = MapCombinator.new`.\n        Arguments:\n            kernel: A single `GenerativeFunction` instance.\n            in_axes: A tuple specifying which `args` to broadcast over.\n            repeats: An integer specifying the length of repetitions (ignored if `in_axes` is specified, if `in_axes` is not specified - required).\n        Returns:\n            instance: A `MapCombinator` instance.\n        \"\"\"\nassert isinstance(kernel, GenerativeFunction)\nif in_axes is None or all(map(lambda v: v is None, in_axes)):\nassert repeats is not None\nreturn MapCombinator(in_axes, repeats, kernel)\ndef _static_broadcast_dim_length(self, args):\ndef find_axis_size(axis, x):\nif axis is not None:\nleaves = jax.tree_util.tree_leaves(x)\nif leaves:\nreturn leaves[0].shape[axis]\nreturn ()\naxis_sizes = jax.tree_util.tree_map(find_axis_size, self.in_axes, args)\naxis_sizes = set(jax.tree_util.tree_leaves(axis_sizes))\nif self.repeats is None and len(axis_sizes) == 1:\n(d_axis_size,) = axis_sizes\nelif len(axis_sizes) &gt; 1:\nraise ValueError(f\"Inconsistent batch axis sizes: {axis_sizes}\")\nelif self.repeats is None:\nraise ValueError(\"repeats should be specified manually.\")\nelse:\nd_axis_size = self.repeats\nreturn d_axis_size\n@typecheck\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nkernel_tt = self.kernel.get_trace_type(*args)\nreturn VectorTraceType(kernel_tt, broadcast_dim_length)\n@typecheck\ndef simulate(\nself, key: PRNGKey, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, VectorTrace]:\n# Argument broadcast semantics must be fully specified\n# in `in_axes`.\nassert len(args) == len(self.in_axes)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindices = np.array([i for i in range(0, broadcast_dim_length)])\nkey, sub_keys = slash(key, broadcast_dim_length)\n_, tr = jax.vmap(self.kernel.simulate, in_axes=(0, self.in_axes))(\nsub_keys, args\n)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = VectorTrace(self, indices, tr, args, retval, jnp.sum(scores))\nreturn key, map_tr\ndef _bounds_checker(self, v, key_len):\nlengths = []\ndef _inner(v):\nif v.shape[-1] &gt; key_len:\nraise Exception(\"Length of leaf longer than max length.\")\nelse:\nlengths.append(v.shape[-1])\nreturn v\nret = jtu.tree_map(_inner, v)\nfixed_len = set(lengths)\nassert len(fixed_len) == 1\nreturn ret, fixed_len.pop()\ndef _static_check_trie_index_compatible(self, chm: Trie, broadcast_dim_length: Int):\nfor (k, _) in chm.get_subtrees_shallow():\nassert isinstance(k, int)\n# TODO: pull outside loop, just check the last address.\nassert k &lt; broadcast_dim_length\ndef _importance_vcm(self, key, chm, args):\ndef _importance(key, chm, args):\nreturn self.kernel.importance(key, chm, args)\ndef _simulate(key, _, args):\nkey, tr = self.kernel.simulate(key, args)\nreturn key, (0.0, tr)\ndef _inner(key, index, chm, args):\ncheck = index == chm.get_index()\nreturn concrete_cond(check, _importance, _simulate, key, chm, args)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindices = np.array([i for i in range(0, broadcast_dim_length)])\nkey, sub_keys = slash(key, broadcast_dim_length)\n_, (w, tr) = jax.vmap(_inner, in_axes=(0, 0, 0, self.in_axes))(\nsub_keys, indices, chm, args\n)\nw = jnp.sum(w)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = VectorTrace(self, indices, tr, args, retval, scores)\nreturn key, (w, map_tr)\n# Implements a conversion from `Trie`.\ndef _importance_tchm(self, key, chm, args):\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nself._static_check_trie_index_compatible(chm, broadcast_dim_length)\n# Okay, so the Trie has an address hierarchy which is compatible with the index structure of the MapCombinator choices.\n# Let's coerce Trie into VectorChoiceMap and then just call `_importance_vcm`.\nvector_chm = self._coerce_to_vector_chm(chm)\nreturn self._importance_vcm(key, vector_chm, args)\ndef _importance_empty(self, key, _, args):\nkey, map_tr = self.simulate(key, args)\nw = 0.0\nreturn key, (w, map_tr)\n@typecheck\ndef importance(\nself,\nkey: PRNGKey,\nchm: Union[EmptyChoiceMap, Trie, VectorChoiceMap],\nargs: Tuple,\n**_,\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, VectorTrace]]:\n# Argument broadcast semantics must be fully specified\n# in `in_axes`.\nassert len(args) == len(self.in_axes)\n# Note: these branches are resolved at tracing time.\nif isinstance(chm, VectorChoiceMap):\nreturn self._importance_vcm(key, chm, args)\nelif isinstance(chm, Trie):\nreturn self._importance_tchm(key, chm, args)\nelse:\nassert isinstance(chm, EmptyChoiceMap)\nreturn self._importance_empty(key, chm, args)\n# The choice map passed in here is a vector choice map.\ndef _update_vcm(\nself, key: PRNGKey, prev: VectorTrace, chm: VectorChoiceMap, argdiffs: Tuple\n):\ndef _update(key, prev, chm, argdiffs):\nkey, (retdiff, w, tr, d) = self.kernel.update(key, prev, chm, argdiffs)\nreturn key, (retdiff, w, tr, d)\ndef _inner(key, index, prev, chm, argdiffs):\ncheck = index == chm.get_index()\nmasked = mask(check, chm.inner)\nreturn _update(key, prev, masked, argdiffs)\n# Just to determine the broadcast length.\nargs = jtu.tree_leaves(argdiffs)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindices = np.array([i for i in range(0, broadcast_dim_length)])\nprev_inaxes_tree = jtu.tree_map(\nlambda v: None if v.shape == () else 0, prev.inner\n)\nkey, sub_keys = slash(key, broadcast_dim_length)\n_, (retdiff, w, tr, discard) = jax.vmap(\n_inner, in_axes=(0, 0, prev_inaxes_tree, 0, self.in_axes)\n)(sub_keys, indices, prev.inner, chm, argdiffs)\nw = jnp.sum(w)\nretval = tr.get_retval()\nscores = tr.get_score()\nmap_tr = VectorTrace(self, indices, tr, args, retval, jnp.sum(scores))\nreturn key, (retdiff, w, map_tr, discard)\n# The choice map passed in here is empty, but perhaps\n# the arguments have changed.\ndef _update_empty(self, key, prev, chm, argdiffs):\ndef _fallback(key, prev, chm, argdiffs):\nkey, (retdiff, w, tr, d) = self.kernel.update(\nkey, prev, EmptyChoiceMap(), argdiffs\n)\nreturn key, (retdiff, w, tr, d)\nprev_inaxes_tree = jtu.tree_map(\nlambda v: None if v.shape == () else 0, prev.inner\n)\n# Just to determine the broadcast length.\nargs = jtu.tree_leaves(argdiffs)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nkey, sub_keys = slash(key, broadcast_dim_length)\n_, (retdiff, w, tr, discard) = jax.vmap(\n_fallback, in_axes=(0, prev_inaxes_tree, 0, self.in_axes)\n)(sub_keys, prev.inner, chm, argdiffs)\nw = jnp.sum(w)\nindices = jnp.array([i for i in range(0, broadcast_dim_length)])\nmap_tr = VectorTrace(self, indices, tr, jnp.sum(tr.get_score()))\nreturn key, (retdiff, w, map_tr, discard)\n@typecheck\ndef update(\nself,\nkey: PRNGKey,\nprev: VectorTrace,\nchm: Union[EmptyChoiceMap, VectorChoiceMap],\nargdiffs: Tuple,\n**_,\n):\n# Argument broadcast semantics must be fully specified\n# in `in_axes`.\nassert len(argdiffs) == len(self.in_axes)\n# Branches here implement certain optimizations when more\n# information about the passed in choice map is available.\nif isinstance(chm, VectorChoiceMap):\nreturn self._update_vcm(key, prev, chm, argdiffs)\nelse:\nassert isinstance(chm, EmptyChoiceMap)\nreturn self._update_empty(key, prev, chm, argdiffs)\n# TODO: I've had so many issues with getting this to work correctly\n# and not throw - and I'm not sure why it's been so finicky.\n# Investigate if it occurs again.\ndef _throw_index_check_host_exception(\nself, check, truth: IntArray, index: IntArray\n):\ndef _inner(args, _):\ntruth = args[0]\nindex = args[1]\ncheck = args[2]\nif not np.all(check):\nraise Exception(\nf\"\\nMapCombinator {self} received a choice map with mismatched indices in assess.\\nReference:\\n{truth}\\nPassed in:\\n{index}\"\n)\nhcb.id_tap(\n_inner,\n(truth, index, check),\nresult=None,\n)\nreturn None\n@typecheck\ndef assess(\nself, key: PRNGKey, chm: VectorChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\n# Argument broadcast semantics must be fully specified\n# in `in_axes`.\nassert len(args) == len(self.in_axes)\nbroadcast_dim_length = self._static_broadcast_dim_length(args)\nindices = jnp.array([i for i in range(0, broadcast_dim_length)])\ncheck = jnp.count_nonzero(indices - chm.get_index()) == 0\n# This inserts a host callback check for bounds checking.\n# If there is an index failure, `assess` must fail\n# because we must provide a constraint for every generative\n# function call.\nself._throw_index_check_host_exception(check, indices, chm.get_index())\ninner = chm.inner\nkey, sub_keys = slash(key, broadcast_dim_length)\n_, (retval, score) = jax.vmap(self.kernel.assess, in_axes=(0, 0, self.in_axes))(\nsub_keys, inner, args\n)\nreturn key, (retval, jnp.sum(score))\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/map.html#genjax._src.generative_functions.combinators.vector.map_combinator.MapCombinator.new","title":"<code>new(kernel, in_axes=None, repeats=None)</code>  <code>classmethod</code>","text":"<p>The preferred constructor for <code>MapCombinator</code> generative function instances. The shorthand symbol is <code>Map = MapCombinator.new</code>.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>GenerativeFunction</code> <p>A single <code>GenerativeFunction</code> instance.</p> required <code>in_axes</code> <code>Union[None, Tuple]</code> <p>A tuple specifying which <code>args</code> to broadcast over.</p> <code>None</code> <code>repeats</code> <code>Union[None, IntArray]</code> <p>An integer specifying the length of repetitions (ignored if <code>in_axes</code> is specified, if <code>in_axes</code> is not specified - required).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>instance</code> <code>MapCombinator</code> <p>A <code>MapCombinator</code> instance.</p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/map_combinator.py</code> <pre><code>@typecheck\n@classmethod\ndef new(\ncls,\nkernel: GenerativeFunction,\nin_axes: Union[None, Tuple] = None,\nrepeats: Union[None, IntArray] = None,\n) -&gt; \"MapCombinator\":\n\"\"\"The preferred constructor for `MapCombinator` generative function\n    instances. The shorthand symbol is `Map = MapCombinator.new`.\n    Arguments:\n        kernel: A single `GenerativeFunction` instance.\n        in_axes: A tuple specifying which `args` to broadcast over.\n        repeats: An integer specifying the length of repetitions (ignored if `in_axes` is specified, if `in_axes` is not specified - required).\n    Returns:\n        instance: A `MapCombinator` instance.\n    \"\"\"\nassert isinstance(kernel, GenerativeFunction)\nif in_axes is None or all(map(lambda v: v is None, in_axes)):\nassert repeats is not None\nreturn MapCombinator(in_axes, repeats, kernel)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/switch.html","title":"Switch combinator","text":"<p>GenJAX's <code>SwitchCombinator</code> is a combinator which branching control flow for generative computation by utilizing <code>jax.lax.switch</code>.</p>"},{"location":"genjax/library/generative_functions/combinators/switch.html#genjax.generative_functions.combinators.SwitchCombinator","title":"<code>genjax.generative_functions.combinators.SwitchCombinator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>GenerativeFunction</code></p> <p><code>SwitchCombinator</code> accepts multiple generative functions as input and implements <code>GenerativeFunction</code> interface semantics that support branching control flow patterns, including control flow patterns which branch on other stochastic choices.</p> <p>Existence uncertainty</p> <p>This pattern allows <code>GenJAX</code> to express existence uncertainty over random choices -- as different generative function branches need not share addresses.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n@genjax.gen\ndef branch_1():\nx = genjax.normal(0.0, 1.0) @ \"x1\"\n@genjax.gen\ndef branch_2():\nx = genjax.bernoulli(0.3) @ \"x2\"\n# Creating a `SwitchCombinator` via the preferred `new` class method.\nswitch = genjax.SwitchCombinator.new(branch_1, branch_2)\nkey = jax.random.PRNGKey(314159)\njitted = jax.jit(genjax.simulate(switch))\nkey, _ = jitted(key, (0, ))\nkey, tr = jitted(key, (1, ))\nprint(console.render(tr))\n</code></pre> <pre><code>SwitchTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 SwitchCombinator\n\u2502       \u2514\u2500\u2500 branches\n\u2502           \u2514\u2500\u2500 list\n\u2502               \u251c\u2500\u2500 BuiltinGenerativeFunction\n\u2502               \u2502   \u2514\u2500\u2500 source\n\u2502               \u2502       \u2514\u2500\u2500 &lt;function branch_1&gt;\n\u2502               \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502                   \u2514\u2500\u2500 source\n\u2502                       \u2514\u2500\u2500 &lt;function branch_2&gt;\n\u251c\u2500\u2500 chm\n\u2502   \u2514\u2500\u2500 SwitchChoiceMap\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u2514\u2500\u2500  i32[]\n\u2502       \u2514\u2500\u2500 submaps\n\u2502           \u2514\u2500\u2500 list\n\u2502               \u251c\u2500\u2500 BuiltinTrace\n\u2502               \u2502   \u251c\u2500\u2500 gen_fn\n\u2502               \u2502   \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502               \u2502   \u2502       \u2514\u2500\u2500 source\n\u2502               \u2502   \u2502           \u2514\u2500\u2500 &lt;function branch_1&gt;\n\u2502               \u2502   \u251c\u2500\u2500 args\n\u2502               \u2502   \u2502   \u2514\u2500\u2500 tuple\n\u2502               \u2502   \u251c\u2500\u2500 retval\n\u2502               \u2502   \u2502   \u2514\u2500\u2500 (const) None\n\u2502               \u2502   \u251c\u2500\u2500 choices\n\u2502               \u2502   \u2502   \u2514\u2500\u2500 Trie\n\u2502               \u2502   \u2502       \u2514\u2500\u2500 :x1\n\u2502               \u2502   \u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502               \u2502   \u2502               \u251c\u2500\u2500 gen_fn\n\u2502               \u2502   \u2502               \u2502   \u2514\u2500\u2500 Normal\n\u2502               \u2502   \u2502               \u251c\u2500\u2500 args\n\u2502               \u2502   \u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502               \u2502   \u2502               \u2502       \u251c\u2500\u2500  f32[]\n\u2502               \u2502   \u2502               \u2502       \u2514\u2500\u2500  f32[]\n\u2502               \u2502   \u2502               \u251c\u2500\u2500 value\n\u2502               \u2502   \u2502               \u2502   \u2514\u2500\u2500  f32[]\n\u2502               \u2502   \u2502               \u2514\u2500\u2500 score\n\u2502               \u2502   \u2502                   \u2514\u2500\u2500  f32[]\n\u2502               \u2502   \u251c\u2500\u2500 cache\n\u2502               \u2502   \u2502   \u2514\u2500\u2500 Trie\n\u2502               \u2502   \u2514\u2500\u2500 score\n\u2502               \u2502       \u2514\u2500\u2500  f32[]\n\u2502               \u2514\u2500\u2500 BuiltinTrace\n\u2502                   \u251c\u2500\u2500 gen_fn\n\u2502                   \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502                   \u2502       \u2514\u2500\u2500 source\n\u2502                   \u2502           \u2514\u2500\u2500 &lt;function branch_2&gt;\n\u2502                   \u251c\u2500\u2500 args\n\u2502                   \u2502   \u2514\u2500\u2500 tuple\n\u2502                   \u251c\u2500\u2500 retval\n\u2502                   \u2502   \u2514\u2500\u2500 (const) None\n\u2502                   \u251c\u2500\u2500 choices\n\u2502                   \u2502   \u2514\u2500\u2500 Trie\n\u2502                   \u2502       \u2514\u2500\u2500 :x2\n\u2502                   \u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502                   \u2502               \u251c\u2500\u2500 gen_fn\n\u2502                   \u2502               \u2502   \u2514\u2500\u2500 Bernoulli\n\u2502                   \u2502               \u251c\u2500\u2500 args\n\u2502                   \u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502                   \u2502               \u2502       \u2514\u2500\u2500  f32[]\n\u2502                   \u2502               \u251c\u2500\u2500 value\n\u2502                   \u2502               \u2502   \u2514\u2500\u2500  bool[]\n\u2502                   \u2502               \u2514\u2500\u2500 score\n\u2502                   \u2502                   \u2514\u2500\u2500  f32[]\n\u2502                   \u251c\u2500\u2500 cache\n\u2502                   \u2502   \u2514\u2500\u2500 Trie\n\u2502                   \u2514\u2500\u2500 score\n\u2502                       \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u251c\u2500\u2500 retval\n\u2502   \u2514\u2500\u2500 (const) None\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/switch/switch_combinator.py</code> <pre><code>@dataclass\nclass SwitchCombinator(GenerativeFunction):\n\"\"\"&gt; `SwitchCombinator` accepts multiple generative functions as input and\n    implements `GenerativeFunction` interface semantics that support branching\n    control flow patterns, including control flow patterns which branch on\n    other stochastic choices.\n    !!! info \"Existence uncertainty\"\n        This pattern allows `GenJAX` to express existence uncertainty over random choices -- as different generative function branches need not share addresses.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        @genjax.gen\n        def branch_1():\n            x = genjax.normal(0.0, 1.0) @ \"x1\"\n        @genjax.gen\n        def branch_2():\n            x = genjax.bernoulli(0.3) @ \"x2\"\n        # Creating a `SwitchCombinator` via the preferred `new` class method.\n        switch = genjax.SwitchCombinator.new(branch_1, branch_2)\n        key = jax.random.PRNGKey(314159)\n        jitted = jax.jit(genjax.simulate(switch))\n        key, _ = jitted(key, (0, ))\n        key, tr = jitted(key, (1, ))\n        print(console.render(tr))\n        ```\n    \"\"\"\nbranches: List[GenerativeFunction]\ndef flatten(self):\nreturn (self.branches,), ()\n@typecheck\n@classmethod\ndef new(cls, *args: GenerativeFunction) -&gt; \"SwitchCombinator\":\n\"\"\"The preferred constructor for `SwitchCombinator` generative function\n        instances. The shorthand symbol is `Switch = SwitchCombinator.new`.\n        Arguments:\n            *args: Generative functions which will act as branch callees for the invocation of branching control flow.\n        Returns:\n            instance: A `SwitchCombinator` instance.\n        \"\"\"\nreturn SwitchCombinator([*args])\n# This overloads the call functionality for this generative function\n# and allows usage of shorthand notation in the builtin DSL.\ndef __call__(self, *args, **kwargs) -&gt; DeferredGenerativeFunctionCall:\nreturn DeferredGenerativeFunctionCall.new(self, args, kwargs)\ndef get_trace_type(self, *args):\nsubtypes = []\nfor gen_fn in self.branches:\nsubtypes.append(gen_fn.get_trace_type(*args[1:]))\nreturn SumTraceType(subtypes)\n# Method is used to create a branch-agnostic type\n# which is acceptable for JAX's typing across `lax.switch`\n# branches.\ndef _create_sum_pytree(self, key, choices, args):\ncovers = []\nfor gen_fn in self.branches:\ntrace_shape = get_trace_data_shape(gen_fn, key, args)\ncovers.append(trace_shape)\nreturn Sumtree.new(choices, covers)\ndef _simulate(self, branch_gen_fn, key, args):\nkey, tr = branch_gen_fn.simulate(key, args[1:])\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nbranch_index = args[0]\nchoice_map = SwitchChoiceMap(branch_index, choices)\nscore = tr.get_score()\nargs = tr.get_args()\nretval = tr.get_retval()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn key, trace\ndef simulate(self, key, args):\nswitch = args[0]\ndef _inner(br):\nreturn lambda key, *args: self._simulate(br, key, args)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, *args)\ndef _importance(self, branch_gen_fn, key, chm, args):\nkey, (w, tr) = branch_gen_fn.importance(key, chm, args[1:])\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nbranch_index = args[0]\nchoice_map = SwitchChoiceMap(branch_index, choices)\nscore = tr.get_score()\nargs = tr.get_args()\nretval = tr.get_retval()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn key, (w, trace)\ndef importance(self, key, chm, args):\nswitch = args[0]\ndef _inner(br):\nreturn lambda key, chm, *args: self._importance(br, key, chm, args)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, chm, *args)\ndef _update(self, branch_gen_fn, key, prev, new, argdiffs):\n# Create a skeleton discard instance.\ndiscard_option = BooleanMask.new(False, prev.strip())\nconcrete_branch_index = self.branches.index(branch_gen_fn)\nargument_index = tree_diff_primal(argdiffs[0])\nmaybe_discard = discard_option.submaps[concrete_branch_index]\n# We have to mask the submap at the concrete_branch_index\n# which we are updating. Why? Because it's possible that the\n# argument_index != concrete_branch_index - meaning we\n# shouldn't perform any inference computations using the submap\n# choices.\nprev = prev.mask_submap(concrete_branch_index, argument_index)\n# Actually perform the update.\nkey, (retval_diff, w, tr, actual_discard) = branch_gen_fn.update(\nkey, prev, new, argdiffs[1:]\n)\n# Here, we create a Sumtree -- and we place the real trace\n# data inside of it.\nargs = jtu.tree_map(tree_diff_primal, argdiffs, is_leaf=static_check_is_diff)\nsum_pytree = self._create_sum_pytree(key, tr, args[1:])\nchoices = list(sum_pytree.materialize_iterator())\nchoice_map = SwitchChoiceMap(concrete_branch_index, choices)\n# Merge the skeleton discard with the actual one.\nactual_discard = maybe_discard.merge(actual_discard)\ndiscard_option.submaps[concrete_branch_index] = actual_discard\n# Get all the metadata for update from the trace.\nscore = tr.get_score()\nargs = tr.get_args()\nretval = tr.get_retval()\ntrace = SwitchTrace(self, choice_map, args, retval, score)\nreturn key, (retval_diff, w, trace, discard_option)\ndef update(self, key, prev, new, argdiffs):\nswitch = tree_diff_primal(argdiffs[0])\ndef _inner(br):\nreturn lambda key, prev, new, argdiffs: self._update(\nbr, key, prev, new, argdiffs\n)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, prev, new, argdiffs)\ndef _assess(self, branch_gen_fn, key, chm, args):\nreturn branch_gen_fn.assess(key, chm, args[1:])\ndef assess(self, key, chm, args):\nswitch = args[0]\ndef _inner(br):\nreturn lambda key, chm, *args: self._assess(br, key, chm, args)\nbranch_functions = list(map(_inner, self.branches))\nreturn jax.lax.switch(switch, branch_functions, key, chm, *args)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/switch.html#genjax._src.generative_functions.combinators.switch.switch_combinator.SwitchCombinator.new","title":"<code>new(*args)</code>  <code>classmethod</code>","text":"<p>The preferred constructor for <code>SwitchCombinator</code> generative function instances. The shorthand symbol is <code>Switch = SwitchCombinator.new</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>GenerativeFunction</code> <p>Generative functions which will act as branch callees for the invocation of branching control flow.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>instance</code> <code>SwitchCombinator</code> <p>A <code>SwitchCombinator</code> instance.</p> Source code in <code>src/genjax/_src/generative_functions/combinators/switch/switch_combinator.py</code> <pre><code>@typecheck\n@classmethod\ndef new(cls, *args: GenerativeFunction) -&gt; \"SwitchCombinator\":\n\"\"\"The preferred constructor for `SwitchCombinator` generative function\n    instances. The shorthand symbol is `Switch = SwitchCombinator.new`.\n    Arguments:\n        *args: Generative functions which will act as branch callees for the invocation of branching control flow.\n    Returns:\n        instance: A `SwitchCombinator` instance.\n    \"\"\"\nreturn SwitchCombinator([*args])\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html","title":"Unfold combinator","text":"<p>GenJAX's <code>UnfoldCombinator</code> is a combinator which implements a scan-like pattern of control flow by utilizing <code>jax.lax.scan</code>.</p>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.UnfoldCombinator","title":"<code>genjax.generative_functions.combinators.UnfoldCombinator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>GenerativeFunction</code></p> <p><code>UnfoldCombinator</code> accepts a kernel generative function, as well as a static maximum unroll length, and provides a scan-like pattern of generative computation.</p> <p>Kernel generative functions</p> <p>A kernel generative function is one which accepts and returns the same signature of arguments. Under the hood, <code>UnfoldCombinator</code> is implemented using <code>jax.lax.scan</code> - which has the same requirements.</p> <p>Examples:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\n# A kernel generative function.\n@genjax.gen\ndef random_walk(prev):\nx = genjax.normal(prev, 1.0) @ \"x\"\nreturn x\n# Creating a `SwitchCombinator` via the preferred `new` class method.\nunfold = genjax.UnfoldCombinator.new(random_walk, 1000)\ninit = 0.5\nkey = jax.random.PRNGKey(314159)\nkey, tr = jax.jit(genjax.simulate(unfold))(key, (999, init))\nprint(console.render(tr))\n</code></pre> <pre><code>VectorTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 UnfoldCombinator\n\u2502       \u251c\u2500\u2500 max_length\n\u2502       \u2502   \u2514\u2500\u2500 (const) 1000\n\u2502       \u2514\u2500\u2500 kernel\n\u2502           \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502               \u2514\u2500\u2500 source\n\u2502                   \u2514\u2500\u2500 &lt;function random_walk&gt;\n\u251c\u2500\u2500 indices\n\u2502   \u2514\u2500\u2500  i32[1000]\n\u251c\u2500\u2500 inner\n\u2502   \u2514\u2500\u2500 BuiltinTrace\n\u2502       \u251c\u2500\u2500 gen_fn\n\u2502       \u2502   \u2514\u2500\u2500 BuiltinGenerativeFunction\n\u2502       \u2502       \u2514\u2500\u2500 source\n\u2502       \u2502           \u2514\u2500\u2500 &lt;function random_walk&gt;\n\u2502       \u251c\u2500\u2500 args\n\u2502       \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502       \u2514\u2500\u2500  f32[1000]\n\u2502       \u251c\u2500\u2500 retval\n\u2502       \u2502   \u2514\u2500\u2500  f32[1000]\n\u2502       \u251c\u2500\u2500 choices\n\u2502       \u2502   \u2514\u2500\u2500 Trie\n\u2502       \u2502       \u2514\u2500\u2500 :x\n\u2502       \u2502           \u2514\u2500\u2500 DistributionTrace\n\u2502       \u2502               \u251c\u2500\u2500 gen_fn\n\u2502       \u2502               \u2502   \u2514\u2500\u2500 Normal\n\u2502       \u2502               \u251c\u2500\u2500 args\n\u2502       \u2502               \u2502   \u2514\u2500\u2500 tuple\n\u2502       \u2502               \u2502       \u251c\u2500\u2500  f32[1000]\n\u2502       \u2502               \u2502       \u2514\u2500\u2500  f32[1000]\n\u2502       \u2502               \u251c\u2500\u2500 value\n\u2502       \u2502               \u2502   \u2514\u2500\u2500  f32[1000]\n\u2502       \u2502               \u2514\u2500\u2500 score\n\u2502       \u2502                   \u2514\u2500\u2500  f32[1000]\n\u2502       \u251c\u2500\u2500 cache\n\u2502       \u2502   \u2514\u2500\u2500 Trie\n\u2502       \u2514\u2500\u2500 score\n\u2502           \u2514\u2500\u2500  f32[1000]\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u251c\u2500\u2500  i32[]\n\u2502       \u2514\u2500\u2500  f32[]\n\u251c\u2500\u2500 retval\n\u2502   \u2514\u2500\u2500  f32[1000]\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/unfold_combinator.py</code> <pre><code>@dataclass\nclass UnfoldCombinator(GenerativeFunction):\n\"\"\"&gt; `UnfoldCombinator` accepts a kernel generative function, as well as a\n    static maximum unroll length, and provides a scan-like pattern of\n    generative computation.\n    !!! info \"Kernel generative functions\"\n        A kernel generative function is one which accepts and returns the same signature of arguments. Under the hood, `UnfoldCombinator` is implemented using `jax.lax.scan` - which has the same requirements.\n    Examples:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        # A kernel generative function.\n        @genjax.gen\n        def random_walk(prev):\n            x = genjax.normal(prev, 1.0) @ \"x\"\n            return x\n        # Creating a `SwitchCombinator` via the preferred `new` class method.\n        unfold = genjax.UnfoldCombinator.new(random_walk, 1000)\n        init = 0.5\n        key = jax.random.PRNGKey(314159)\n        key, tr = jax.jit(genjax.simulate(unfold))(key, (999, init))\n        print(console.render(tr))\n        ```\n    \"\"\"\nmax_length: IntArray\nkernel: GenerativeFunction\ndef flatten(self):\nreturn (self.kernel,), (self.max_length,)\n@typecheck\n@classmethod\ndef new(cls, kernel: GenerativeFunction, max_length: Int) -&gt; \"UnfoldCombinator\":\n\"\"\"The preferred constructor for `UnfoldCombinator` generative function\n        instances. The shorthand symbol is `Unfold = UnfoldCombinator.new`.\n        Arguments:\n            kernel: A kernel `GenerativeFunction` instance.\n            max_length: A static maximum possible unroll length.\n        Returns:\n            instance: An `UnfoldCombinator` instance.\n        \"\"\"\nreturn UnfoldCombinator(max_length, kernel)\n# This overloads the call functionality for this generative function\n# and allows usage of shorthand notation in the builtin DSL.\ndef __call__(self, *args, **kwargs) -&gt; DeferredGenerativeFunctionCall:\nreturn DeferredGenerativeFunctionCall.new(self, args, kwargs)\n@typecheck\ndef get_trace_type(self, *args, **kwargs) -&gt; VectorTraceType:\n_ = args[0]\nargs = args[1:]\ninner_type = self.kernel.get_trace_type(*args, **kwargs)\nreturn VectorTraceType(inner_type, self.max_length)\ndef _throw_bounds_host_exception(self, count: int):\ndef _inner(count, _):\nraise Exception(\nf\"\\nUnfoldCombinator {self} received a length argument ({count}) longer than specified max length ({self.max_length})\"\n)\nhcb.id_tap(\nlambda *args: _inner(*args),\ncount,\nresult=None,\n)\nreturn None\n@typecheck\ndef simulate(self, key: PRNGKey, args: Tuple, **_) -&gt; Tuple[PRNGKey, VectorTrace]:\nlength = args[0]\nstate = args[1]\nstatic_args = args[2:]\n# This inserts a host callback check for bounds checking.\ncheck = jnp.less(self.max_length, length + 1)\nconcrete_cond(\ncheck,\nlambda *_: self._throw_bounds_host_exception(length + 1),\nlambda *_: None,\n)\nzero_trace = make_zero_trace(\nself.kernel,\nkey,\n(state, *static_args),\n)\ndef _inner_simulate(key, state, static_args, count):\nkey, tr = self.kernel.simulate(key, (state, *static_args))\nstate = tr.get_retval()\nscore = tr.get_score()\nreturn (key, tr, state, count, count + 1, score)\ndef _inner_zero_fallback(key, state, _, count):\nstate = state\nscore = 0.0\nreturn (key, zero_trace, state, -1, count, score)\ndef _inner(carry, _):\ncount, key, state = carry\ncheck = jnp.less(count, length + 1)\nkey, tr, state, index, count, score = concrete_cond(\ncheck,\n_inner_simulate,\n_inner_zero_fallback,\nkey,\nstate,\nstatic_args,\ncount,\n)\nreturn (count, key, state), (tr, index, state, score)\n(_, key, state), (tr, indices, retval, scores) = jax.lax.scan(\n_inner,\n(0, key, state),\nNone,\nlength=self.max_length,\n)\nunfold_tr = VectorTrace(self, indices, tr, args, retval, jnp.sum(scores))\nreturn key, unfold_tr\n# This checks the leaves of a choice map,\n# to determine if it is \"out of bounds\" for\n# the max static length of this combinator.\ndef _static_bounds_check(self, v):\nlengths = []\ndef _inner(v):\nif v.shape[-1] &gt; self.max_length:\nraise Exception(\"Length of leaf longer than max length.\")\nelse:\nlengths.append(v.shape[-1])\nreturn v\nret = jtu.tree_map(_inner, v)\nfixed_len = set(lengths)\nassert len(fixed_len) == 1\nreturn ret, fixed_len.pop()\ndef _importance_indexed(self, key, chm, args):\nlength = args[0]\nstate = args[1]\nstatic_args = args[2:]\n# Unwrap the index mask.\ninner_choice_map = chm.inner\ntarget_index = chm.get_index()\n# Complicated - refactor in future.\ndef _inner(carry, _):\ncount, key, state = carry\ndef _importance(key, state):\nreturn self.kernel.importance(\nkey, inner_choice_map, (state, *static_args)\n)\ndef _simulate(key, state):\nkey, tr = self.kernel.simulate(key, (state, *static_args))\nreturn key, (0.0, tr)\ncheck = count == target_index\nkey, (w, tr) = concrete_cond(check, _importance, _simulate, key, state)\ncheck = jnp.less(count, length + 1)\nindex = concrete_cond(check, lambda *_: count, lambda *_: -1)\ncount, state, score, w = concrete_cond(\ncheck,\nlambda *args: (count + 1, tr.get_retval(), tr.get_score(), w),\nlambda *args: (count, state, 0.0, 0.0),\n)\nreturn (count, key, state), (w, score, tr, index, state)\n(count, key, state), (w, score, tr, indices, retval) = jax.lax.scan(\n_inner,\n(0, key, state),\nNone,\nlength=self.max_length,\n)\nunfold_tr = VectorTrace(self, indices, tr, args, retval, jnp.sum(score))\nw = jnp.sum(w)\nreturn key, (w, unfold_tr)\ndef _importance_vcm(self, key, chm, args):\nlength = args[0]\nstate = args[1]\nstatic_args = args[2:]\ndef _inner(carry, slice):\ncount, key, state = carry\nchm = slice\ndef _importance(key, chm, state):\nreturn self.kernel.importance(key, chm, (state, *static_args))\ndef _simulate(key, chm, state):\nkey, tr = self.kernel.simulate(key, (state, *static_args))\nreturn key, (0.0, tr)\ncheck = count == chm.get_index()\nkey, (w, tr) = concrete_cond(\ncheck,\n_importance,\n_simulate,\nkey,\nchm,\nstate,\n)\ncheck = jnp.less(count, length + 1)\nindex = concrete_cond(\ncheck,\nlambda *args: count,\nlambda *args: -1,\n)\ncount, state, score, w = concrete_cond(\ncheck,\nlambda *args: (\ncount + 1,\ntr.get_retval(),\ntr.get_score(),\nw,\n),\nlambda *args: (count, state, 0.0, 0.0),\n)\nreturn (count, key, state), (w, score, tr, index, state)\n(count, key, state), (w, score, tr, indices, retval) = jax.lax.scan(\n_inner,\n(0, key, state),\nchm,\nlength=self.max_length,\n)\nunfold_tr = VectorTrace(self, indices, tr, args, retval, jnp.sum(score))\nw = jnp.sum(w)\nreturn key, (w, unfold_tr)\ndef _importance_empty(self, key, _, args):\nkey, unfold_tr = self.simulate(key, args)\nw = 0.0\nreturn key, (w, unfold_tr)\n@typecheck\ndef importance(\nself, key: PRNGKey, chm: ChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, VectorTrace]]:\nlength = args[0]\n# This inserts a host callback check for bounds checking.\n# At runtime, if the bounds are exceeded -- an error\n# will be emitted.\ncheck = jnp.less(self.max_length, length + 1)\nconcrete_cond(\ncheck,\nlambda *args: self._throw_bounds_host_exception(length + 1),\nlambda *args: None,\n)\nif isinstance(chm, VectorChoiceMap):\nreturn self._importance_vcm(key, chm, args)\nelse:\nassert isinstance(chm, EmptyChoiceMap)\nreturn self._importance_empty(key, chm, args)\n# The choice map is a vector choice map.\ndef _update_vcm(self, key, prev, chm, argdiffs):\nlength = argdiffs[0]\nstate = argdiffs[1]\nstatic_args = argdiffs[2:]\nargs = tree_diff_primal(argdiffs)\ndef _inner(carry, slice):\ncount, key, state = carry\n(prev, chm) = slice\ndef _update(key, prev, chm, state):\nreturn self.kernel.update(key, prev, chm, (state, *static_args))\ndef _fallthrough(key, prev, chm, state):\nreturn self.kernel.update(\nkey, prev, EmptyChoiceMap(), (state, *static_args)\n)\ncheck = count == chm.get_index()\nkey, (retdiff, w, tr, discard) = concrete_cond(\ncheck, _update, _fallthrough, key, prev, chm, state\n)\ncheck = jnp.less(count, length + 1)\nindex = concrete_cond(check, lambda *args: count, lambda *args: -1)\ncount, state, score, weight = concrete_cond(\ncheck,\nlambda *args: (count + 1, retdiff, tr.get_score(), w),\nlambda *args: (count, state, 0.0, 0.0),\n)\nreturn (count, key, state), (state, score, w, tr, discard, index)\n(count, key, state), (\nretdiff,\nscore,\nw,\ntr,\ndiscard,\nindices,\n) = jax.lax.scan(_inner, (0, key, state), (prev, chm), length=self.max_length)\nunfold_tr = VectorTrace(\nself, indices, tr, args, retdiff.get_val(), jnp.sum(score)\n)\nw = jnp.sum(w)\nreturn key, (retdiff, w, unfold_tr, discard)\n@typecheck\ndef update(\nself,\nkey: PRNGKey,\nprev: Trace,\nchm: Union[EmptyChoiceMap, VectorChoiceMap],\nargdiffs: Tuple,\n**_,\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, VectorTrace, ChoiceMap]]:\nlength = argdiffs[0].get_val()\n# Unwrap the previous trace at this address\n# we should get a `VectorChoiceMap`.\n# We don't need the index indicators from the trace,\n# so we can just unwrap it.\nprev = prev.inner\n# This inserts a host callback check for bounds checking.\n# If we go out of bounds on device, it throws to the\n# Python runtime -- which will raise.\ncheck = jnp.less(self.max_length, length + 1)\nconcrete_cond(\ncheck,\nlambda *args: self._throw_bounds_host_exception(length + 1),\nlambda *args: None,\n)\nif isinstance(chm, VectorChoiceMap):\nreturn self._update_vcm(key, prev, chm, argdiffs)\nelse:\nreturn self._update_empty(key, prev, chm, argdiffs)\ndef _throw_index_check_host_exception(self, count: IntArray, index: IntArray):\ndef _inner(pair, transforms):\n(count, index) = pair\nraise Exception(\nf\"\\nUnfoldCombinator {self} received a choice map with mismatched indices (count {count}, at index {index}) in assess.\"\n)\nhcb.id_tap(\nlambda *args: _inner(*args),\n(count, index),\nresult=None,\n)\nreturn None\n@typecheck\ndef assess(\nself, key: PRNGKey, chm: ChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\nassert isinstance(chm, VectorChoiceMap)\nlength = args[0]\n# This inserts a host callback check for bounds checking.\n# At runtime, if the bounds are exceeded -- an error\n# will be emitted.\ncheck = jnp.less(self.max_length, length + 1)\nconcrete_cond(\ncheck,\nlambda *args: self._throw_bounds_host_exception(length + 1),\nlambda *args: None,\n)\nlength = args[0]\nstate = args[1]\nstatic_args = args[2:]\ndef _inner(carry, slice):\ncount, key, state = carry\nchm = slice\ncheck = count == chm.get_index()\n# TODO: the below doesn't work when `vmap` is used.\n# This inserts a host callback check for bounds checking.\n# If there is an index failure, `assess` must fail\n# because we must provide a constraint for every generative\n# function call.\n# jax.lax.cond(\n#    check,\n#    lambda *args: None,\n#    lambda *args: self._throw_index_check_host_exception(*args),\n#    count,\n#    chm.get_index(),\n# )\nkey, (retval, score) = self.kernel.assess(key, chm, (state, *static_args))\ncheck = jnp.less(count, length + 1)\nindex = concrete_cond(\ncheck,\nlambda *args: count,\nlambda *args: -1,\n)\ncount, state, score = concrete_cond(\ncheck,\nlambda *args: (count + 1, retval, score),\nlambda *args: (count, state, 0.0),\n)\nreturn (count, key, state), (state, score, index)\n(_, key, state), (retval, score, _) = jax.lax.scan(\n_inner,\n(0, key, state),\nchm,\nlength=self.max_length,\n)\nscore = jnp.sum(score)\nreturn key, (retval, score)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax._src.generative_functions.combinators.vector.unfold_combinator.UnfoldCombinator.new","title":"<code>new(kernel, max_length)</code>  <code>classmethod</code>","text":"<p>The preferred constructor for <code>UnfoldCombinator</code> generative function instances. The shorthand symbol is <code>Unfold = UnfoldCombinator.new</code>.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>GenerativeFunction</code> <p>A kernel <code>GenerativeFunction</code> instance.</p> required <code>max_length</code> <code>Int</code> <p>A static maximum possible unroll length.</p> required <p>Returns:</p> Name Type Description <code>instance</code> <code>UnfoldCombinator</code> <p>An <code>UnfoldCombinator</code> instance.</p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/unfold_combinator.py</code> <pre><code>@typecheck\n@classmethod\ndef new(cls, kernel: GenerativeFunction, max_length: Int) -&gt; \"UnfoldCombinator\":\n\"\"\"The preferred constructor for `UnfoldCombinator` generative function\n    instances. The shorthand symbol is `Unfold = UnfoldCombinator.new`.\n    Arguments:\n        kernel: A kernel `GenerativeFunction` instance.\n        max_length: A static maximum possible unroll length.\n    Returns:\n        instance: An `UnfoldCombinator` instance.\n    \"\"\"\nreturn UnfoldCombinator(max_length, kernel)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#choice-maps-for-unfold","title":"Choice maps for <code>Unfold</code>","text":"<p><code>Unfold</code> produces <code>VectorChoiceMap</code> instances (a type of choice map shared with <code>MapCombinator</code>).</p> <p>To utilize <code>importance</code>, <code>update</code>, or <code>assess</code> with <code>Unfold</code>, it suffices to provide either a <code>VectorChoiceMap</code> for constraints, or an <code>IndexChoiceMap</code>. Both of these choice maps are documented below (documentation is mirrored at <code>MapCombinator</code>).</p>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.VectorChoiceMap","title":"<code>genjax.generative_functions.combinators.VectorChoiceMap</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass VectorChoiceMap(ChoiceMap):\nindices: IntArray\ninner: Union[ChoiceMap, Trace]\ndef flatten(self):\nreturn (self.indices, self.inner), ()\n@classmethod\ndef _static_check_broadcast_dim_length(cls, tree):\nbroadcast_dim_tree = jtu.tree_map(lambda v: len(v), tree)\nleaves = jtu.tree_leaves(broadcast_dim_tree)\nleaf_lengths = set(leaves)\n# all the leaves must have the same first dim size.\nassert len(leaf_lengths) == 1\nmax_index = list(leaf_lengths).pop()\nreturn max_index\n@typecheck\n@classmethod\ndef new(cls, indices, inner: ChoiceMap) -&gt; ChoiceMap:\n# if you try to wrap around an EmptyChoiceMap, do nothing.\nif isinstance(inner, EmptyChoiceMap):\nreturn inner\n# convert list to array.\nif isinstance(indices, list):\n# indices can't be empty.\nassert indices\nindices = jnp.array(indices)\nindices_len = len(indices)\ninner_len = VectorChoiceMap._static_check_broadcast_dim_length(inner)\n# indices must have same length as leaves of the inner choice map.\nassert indices_len == inner_len\nreturn VectorChoiceMap(indices, inner)\n@typecheck\n@classmethod\ndef convert(cls, choice_map: Trie):\nindices = []\nsubtrees = []\nfor (ind, subtree) in choice_map.get_subtrees_shallow():\nindices.append(ind)\nsubtrees.append(subtree)\n# Assert that all Pytrees in list can be\n# stacked at leaves.\n# static_check_pytree_stackable(subtrees)\n# return VectorChoiceMap.new(indices, tree_stack(subtrees))\ndef get_selection(self):\nsubselection = self.inner.get_selection()\nreturn VectorSelection.new(subselection)\ndef has_subtree(self, addr):\nreturn self.inner.has_subtree(addr)\ndef get_subtree(self, addr):\nreturn self.inner.get_subtree(addr)\ndef get_subtrees_shallow(self):\nreturn self.inner.get_subtrees_shallow()\n# TODO: This currently provides poor support for merging\n# two vector choices maps with different index arrays.\ndef merge(self, other):\nif isinstance(other, VectorChoiceMap):\nreturn VectorChoiceMap(other.indices, self.inner.merge(other.inner))\nelse:\nreturn VectorChoiceMap(self.indices, self.inner.merge(other))\ndef __hash__(self):\nreturn hash(self.inner)\ndef get_index(self):\nreturn self.indices\ndef _tree_console_overload(self):\ntree = Tree(f\"[b]{self.__class__.__name__}[/b]\")\nsubt = self.inner._build_rich_tree()\nsubk = Tree(\"[blue]indices\")\nsubk.add(gpp.tree_pformat(self.indices))\ntree.add(subk)\ntree.add(subt)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax._src.generative_functions.combinators.vector.vector_datatypes.VectorChoiceMap.new","title":"<code>new(indices, inner)</code>  <code>classmethod</code>","text":"Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@typecheck\n@classmethod\ndef new(cls, indices, inner: ChoiceMap) -&gt; ChoiceMap:\n# if you try to wrap around an EmptyChoiceMap, do nothing.\nif isinstance(inner, EmptyChoiceMap):\nreturn inner\n# convert list to array.\nif isinstance(indices, list):\n# indices can't be empty.\nassert indices\nindices = jnp.array(indices)\nindices_len = len(indices)\ninner_len = VectorChoiceMap._static_check_broadcast_dim_length(inner)\n# indices must have same length as leaves of the inner choice map.\nassert indices_len == inner_len\nreturn VectorChoiceMap(indices, inner)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax.generative_functions.combinators.IndexChoiceMap","title":"<code>genjax.generative_functions.combinators.IndexChoiceMap</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ChoiceMap</code></p> Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@dataclass\nclass IndexChoiceMap(ChoiceMap):\nindex: Int\ninner: ChoiceMap\ndef flatten(self):\nreturn (self.index, self.inner), ()\n@typecheck\n@classmethod\ndef new(cls, index, inner: ChoiceMap) -&gt; ChoiceMap:\n# if you try to wrap around an EmptyChoiceMap, do nothing.\nif isinstance(inner, EmptyChoiceMap):\nreturn inner\nreturn IndexChoiceMap(index, inner)\ndef get_selection(self):\nsubselection = self.inner.get_selection()\nreturn IndexSelection.new(subselection)\ndef has_subtree(self, addr):\nreturn self.inner.has_subtree(addr)\ndef get_subtree(self, addr):\nreturn self.inner.get_subtree(addr)\ndef get_subtrees_shallow(self):\nreturn self.inner.get_subtrees_shallow()\n# TODO: This currently provides poor support for merging\n# two vector choices maps with different index arrays.\ndef merge(self, other):\nif isinstance(other, VectorChoiceMap):\nreturn VectorChoiceMap(other.indices, self.inner.merge(other.inner))\nelse:\nreturn VectorChoiceMap(self.indices, self.inner.merge(other))\ndef __hash__(self):\nreturn hash(self.inner)\ndef get_index(self):\nreturn self.indices\ndef _tree_console_overload(self):\ntree = Tree(f\"[b]{self.__class__.__name__}[/b]\")\nsubt = self.inner._build_rich_tree()\nsubk = Tree(\"[blue]indices\")\nsubk.add(gpp.tree_pformat(self.indices))\ntree.add(subk)\ntree.add(subt)\nreturn tree\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#genjax._src.generative_functions.combinators.vector.vector_datatypes.IndexChoiceMap.new","title":"<code>new(index, inner)</code>  <code>classmethod</code>","text":"Source code in <code>src/genjax/_src/generative_functions/combinators/vector/vector_datatypes.py</code> <pre><code>@typecheck\n@classmethod\ndef new(cls, index, inner: ChoiceMap) -&gt; ChoiceMap:\n# if you try to wrap around an EmptyChoiceMap, do nothing.\nif isinstance(inner, EmptyChoiceMap):\nreturn inner\nreturn IndexChoiceMap(index, inner)\n</code></pre>"},{"location":"genjax/library/generative_functions/combinators/unfold.html#selections-for-vectorchoicemap","title":"Selections for <code>VectorChoiceMap</code>","text":"<p>(This section is also mirrored for <code>MapCombinator</code>)</p> <p>To select from <code>VectorChoiceMap</code>, both <code>VectorSelection</code> and <code>IndexSelection</code> can be used. <code>VectorSelection</code></p>"},{"location":"genjax/library/generative_functions/distributions/index.html","title":"Distributions","text":"<p>Info</p> <p>On this page, we document the abstract base classes which are used throughout the module. For submodules which implement distributions using the base classes (e.g. <code>scipy</code>, or <code>tfd</code>) - we list the available distributions.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions","title":"<code>genjax._src.generative_functions.distributions</code>","text":"<p>This module provides:</p> <ul> <li> <p>Abstract base classes for declaring distributions as <code>GenerativeFunction</code> types. These classes include <code>Distribution</code> and <code>ExactDensity</code>. The latter assumes that the inheritor exposes exact density evaluation, while the former makes no such assumption.</p> </li> <li> <p>Several distributions from JAX's <code>scipy</code> module, as well as TensorFlow Distributions (<code>tfd</code>) from TensorFlow Probability (<code>tfp</code>) using the JAX backend.</p> </li> <li> <p>Custom distributions, including ones with exact posteriors (like discrete HMMs).</p> </li> <li> <p>A language (<code>coryx</code>) based on <code>oryx</code> for defining new distribution objects from inverse log determinant Jacobian transformations on existing distributions.</p> </li> <li> <p>A language (<code>gensp</code>) for defining distributions with estimated densities using inference.</p> </li> </ul>"},{"location":"genjax/library/generative_functions/distributions/index.html#the-distribution-abstract-base-class","title":"The <code>Distribution</code> abstract base class","text":""},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.Distribution","title":"<code>genjax.generative_functions.distributions.Distribution</code>  <code>dataclass</code>","text":"<p>         Bases: <code>GenerativeFunction</code></p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@dataclass\nclass Distribution(GenerativeFunction):\ndef flatten(self):\nreturn (), ()\n# This overloads the call functionality for this generative function\n# and allows usage of shorthand notation in the builtin DSL.\ndef __call__(self, *args, **kwargs) -&gt; DeferredGenerativeFunctionCall:\nreturn DeferredGenerativeFunctionCall.new(self, args, kwargs)\n# Syntactical overload to define `Product` of distributions.\n# C.f. below.\ndef __mul__(self, other):\np = Product([])\np.append(self)\np.append(other)\nreturn p\n@typecheck\ndef get_trace_type(self, *args, **kwargs) -&gt; TraceType:\n# `get_trace_type` is compile time - the key value\n# doesn't matter, just the type.\nkey = jax.random.PRNGKey(1)\n_, (_, (_, ttype)) = jax.make_jaxpr(self.random_weighted, return_shape=True)(\nkey, *args\n)\nreturn tt_lift(ttype)\n@abc.abstractmethod\ndef random_weighted(self, *args, **kwargs):\npass\n@abc.abstractmethod\ndef estimate_logpdf(self, key, v, *args, **kwargs):\npass\n@typecheck\ndef simulate(\nself, key: PRNGKey, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, DistributionTrace]:\nkey, (w, v) = self.random_weighted(key, *args, **kwargs)\ntr = DistributionTrace(self, args, v, w)\nreturn key, tr\n@typecheck\ndef importance(\nself, key: PRNGKey, chm: ChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[FloatArray, DistributionTrace]]:\nassert isinstance(chm, Leaf)\n# If the choice map is empty, we just simulate\n# and return 0.0 for the log weight.\nif isinstance(chm, EmptyChoiceMap):\nkey, tr = self.simulate(key, args, **kwargs)\nreturn key, (0.0, tr)\n# If it's not empty, we should check if it is a mask.\n# If it is a mask, we need to see if it is active or not,\n# and then unwrap it - and use the active flag to determine\n# what to do at runtime.\nv = chm.get_leaf_value()\nif isinstance(v, BooleanMask):\nactive = v.mask\nv = v.unmask()\ndef _active(key, v, args):\nkey, w = self.estimate_logpdf(key, v, *args)\nreturn key, v, w\ndef _inactive(key, v, _):\nw = 0.0\nreturn key, v, w\nkey, v, w = concrete_cond(active, _active, _inactive, key, v, args)\nscore = w\n# Otherwise, we just estimate the logpdf of the value\n# we got out of the choice map.\nelse:\nkey, w = self.estimate_logpdf(key, v, *args)\nscore = w\nreturn key, (\nw,\nDistributionTrace(self, args, v, score),\n)\n# NOTE: Here's an interesting note about `update`...\n# (really, any of the GFI methods for any generative function)\n# - they should return homogeneous types for any return\n# branch leading out of the call.\n# Because these methods may be invoked in `jax.lax.switch` calls\n# it's important that callers have some knowledge about the\n# consistency of invoking a callee -- most generative function\n# languages ensure this is true by default e.g. if they defer\n# some of their behavior to callees.\n# For `Distribution` this is not true by default - we have to be\n# careful when defining the methods, and this is most true of update\n# below.\n@typecheck\ndef update(\nself,\nkey: PRNGKey,\nprev: DistributionTrace,\nconstraints: ChoiceMap,\nargdiffs: Tuple,\n**kwargs\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray, DistributionTrace, Any]]:\nassert isinstance(constraints, Leaf)\nmaybe_discard = mask(False, prev.get_choices())\n# Incremental optimization - if nothing has changed,\n# just return the previous trace.\nif isinstance(constraints, EmptyChoiceMap) and static_check_no_change(argdiffs):\nv = prev.get_retval()\nretval_diff = jtu.tree_map(lambda v: Diff(v, NoChange), v)\nreturn key, (retval_diff, 0.0, prev, maybe_discard)\n# Otherwise, we consider the cases.\nargs = tree_diff_primal(argdiffs)\n# First, we have to check if the trace provided\n# is masked or not. It's possible that a trace\n# with a mask is updated.\nprev_v = prev.get_retval()\nactive = True\nif isinstance(prev_v, BooleanMask):\nactive = prev_v.mask\nprev_v = prev_v.unmask()\n# Case 1: the new choice map is empty here.\nif isinstance(constraints, EmptyChoiceMap):\nprev_score = prev.get_score()\nv = prev_v\n# If the value is active, we compute any weight\n# corrections from changing arguments.\ndef _active(key, v, *args):\nkey, fwd = self.estimate_logpdf(key, v, *args)\nreturn key, fwd - prev_score\n# If the value is inactive, we do nothing.\ndef _inactive(key, v, *args):\nreturn key, prev_score\nkey, w = concrete_cond(active, _active, _inactive, key, v, *args)\ndiscard = maybe_discard\nretval_diff = jtu.tree_map(lambda v: Diff(v, NoChange), prev_v)\n# Case 2: the new choice map is not empty here.\nelse:\nprev_score = prev.get_score()\nv = constraints.get_leaf_value()\n# Now, we must check if the choice map has a masked\n# leaf value, and dispatch accordingly.\nactive_chm = True\nif isinstance(v, BooleanMask):\nactive_chm = v.mask\nv = v.unmask()\n# The only time this flag is on is when both leaf values\n# are concrete, or they are both masked with true mask\n# values.\nactive = jnp.all(jnp.logical_and(active_chm, active))\nkey, fwd = self.estimate_logpdf(key, v, *args)\ndef _constraints_active(key, v, *args):\nreturn key, v, fwd - prev_score\ndef _constraints_inactive(key, v, *args):\nreturn key, prev_v, 0.0\nkey, v, w = concrete_cond(\nactive_chm, _constraints_active, _constraints_inactive, key, v, *args\n)\ndiscard = mask(active_chm, ValueChoiceMap(prev.get_leaf_value()))\nretval_diff = jtu.tree_map(lambda v: Diff(v, UnknownChange), v)\nreturn key, (\nretval_diff,\nw,\nDistributionTrace(self, args, v, w),\ndiscard,\n)\n@typecheck\ndef assess(\nself, key: PRNGKey, evaluation_point: ValueChoiceMap, args: Tuple, **kwargs\n) -&gt; Tuple[PRNGKey, Tuple[Any, FloatArray]]:\nv = evaluation_point.get_leaf_value()\nkey, score = self.estimate_logpdf(key, v, *args)\nreturn key, (v, score)\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.Distribution.random_weighted","title":"<code>random_weighted(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef random_weighted(self, *args, **kwargs):\npass\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.Distribution.estimate_logpdf","title":"<code>estimate_logpdf(key, v, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef estimate_logpdf(self, key, v, *args, **kwargs):\npass\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#the-exactdensity-abstract-base-class","title":"The <code>ExactDensity</code> abstract base class","text":"<p>If you are attempting to create a new <code>Distribution</code>, you'll likely want to inherit from <code>ExactDensity</code> - which assumes that you have access to an exact logpdf method (a more restrictive assumption than <code>Distribution</code>). This is most often the case: all of the standard distributions (<code>scipy</code>, <code>tfd</code>) use <code>ExactDensity</code>.</p>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax.generative_functions.distributions.ExactDensity","title":"<code>genjax.generative_functions.distributions.ExactDensity</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Distribution</code></p> <p>Abstract base class which extends Distribution and assumes that the implementor provides an exact logpdf method (compared to one which returns an estimate of the logpdf).</p> <p>All of the standard distributions inherit from <code>ExactDensity</code>, and if you are looking to implement your own distribution, you should likely use this class.</p> <p><code>Distribution</code> implementors are <code>Pytree</code> implementors</p> <p>As <code>Distribution</code> extends <code>Pytree</code>, if you use this class, you must implement <code>flatten</code> as part of your class declaration.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@dataclass\nclass ExactDensity(Distribution):\n\"\"\"&gt; Abstract base class which extends Distribution and assumes that the\n    implementor provides an exact logpdf method (compared to one which returns\n    _an estimate of the logpdf_).\n    All of the standard distributions inherit from `ExactDensity`, and\n    if you are looking to implement your own distribution, you should\n    likely use this class.\n    !!! info \"`Distribution` implementors are `Pytree` implementors\"\n    As `Distribution` extends `Pytree`, if you use this class, you must\n    implement `flatten` as part of your class declaration.\n    \"\"\"\n@abc.abstractmethod\ndef sample(self, key: PRNGKey, *args, **kwargs) -&gt; Any:\n\"\"\"&gt; Sample from the distribution, returning a value from the event\n        space.\n        Arguments:\n            key: A `PRNGKey`.\n            *args: The arguments to the distribution invocation.\n        Returns:\n            v: A value from the event space of the distribution.\n        !!! info \"Implementations need not return a new `PRNGKey`\"\n            Note that `sample` does not return a new evolved `PRNGKey`. This is for convenience - `ExactDensity` is used often, and the interface for `sample` is simple. `sample` is called by `random_weighted` in the generative function interface implementations, and always gets a fresh `PRNGKey` - `sample` as a callee need not return a new evolved key.\n        Examples:\n            `genjax.normal` is a distribution with an exact density, which supports the `sample` interface. Here's an example of invoking `sample`.\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            v = genjax.normal.sample(key, 0.0, 1.0)\n            print(console.render(v))\n            ```\n            Note that you often do want or need to invoke `sample` directly - you'll likely want to use the generative function interface methods instead:\n            ```python exec=\"yes\" source=\"tabbed-left\"\n            import jax\n            import genjax\n            console = genjax.pretty()\n            key = jax.random.PRNGKey(314159)\n            key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n            print(console.render(tr))\n            ```\n        \"\"\"\n@abc.abstractmethod\ndef logpdf(self, v, *args, **kwargs):\n\"\"\"&gt; Given a value from the event space, compute the log probability of\n        that value under the distribution.\"\"\"\ndef random_weighted(self, key, *args, **kwargs):\nkey, sub_key = jax.random.split(key)\nv = self.sample(sub_key, *args, **kwargs)\nw = self.logpdf(v, *args, **kwargs)\nreturn key, (w, v)\ndef estimate_logpdf(self, key, v, *args, **kwargs):\nw = self.logpdf(v, *args, **kwargs)\nreturn key, w\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.ExactDensity.sample","title":"<code>sample(key, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Sample from the distribution, returning a value from the event space.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A <code>PRNGKey</code>.</p> required <code>*args</code> <p>The arguments to the distribution invocation.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>v</code> <code>Any</code> <p>A value from the event space of the distribution.</p> <p>Implementations need not return a new <code>PRNGKey</code></p> <p>Note that <code>sample</code> does not return a new evolved <code>PRNGKey</code>. This is for convenience - <code>ExactDensity</code> is used often, and the interface for <code>sample</code> is simple. <code>sample</code> is called by <code>random_weighted</code> in the generative function interface implementations, and always gets a fresh <code>PRNGKey</code> - <code>sample</code> as a callee need not return a new evolved key.</p> <p>Examples:</p> <p><code>genjax.normal</code> is a distribution with an exact density, which supports the <code>sample</code> interface. Here's an example of invoking <code>sample</code>.</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nv = genjax.normal.sample(key, 0.0, 1.0)\nprint(console.render(v))\n</code></pre> <pre><code>-0.62537825\n</code></pre> <p>Note that you often do want or need to invoke <code>sample</code> directly - you'll likely want to use the generative function interface methods instead:</p> SourceResult <pre><code>import jax\nimport genjax\nconsole = genjax.pretty()\nkey = jax.random.PRNGKey(314159)\nkey, tr = genjax.normal.simulate(key, (0.0, 1.0))\nprint(console.render(tr))\n</code></pre> <pre><code>DistributionTrace\n\u251c\u2500\u2500 gen_fn\n\u2502   \u2514\u2500\u2500 Normal\n\u251c\u2500\u2500 args\n\u2502   \u2514\u2500\u2500 tuple\n\u2502       \u251c\u2500\u2500 (const) 0.0\n\u2502       \u2514\u2500\u2500 (const) 1.0\n\u251c\u2500\u2500 value\n\u2502   \u2514\u2500\u2500  f32[]\n\u2514\u2500\u2500 score\n    \u2514\u2500\u2500  f32[]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef sample(self, key: PRNGKey, *args, **kwargs) -&gt; Any:\n\"\"\"&gt; Sample from the distribution, returning a value from the event\n    space.\n    Arguments:\n        key: A `PRNGKey`.\n        *args: The arguments to the distribution invocation.\n    Returns:\n        v: A value from the event space of the distribution.\n    !!! info \"Implementations need not return a new `PRNGKey`\"\n        Note that `sample` does not return a new evolved `PRNGKey`. This is for convenience - `ExactDensity` is used often, and the interface for `sample` is simple. `sample` is called by `random_weighted` in the generative function interface implementations, and always gets a fresh `PRNGKey` - `sample` as a callee need not return a new evolved key.\n    Examples:\n        `genjax.normal` is a distribution with an exact density, which supports the `sample` interface. Here's an example of invoking `sample`.\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        v = genjax.normal.sample(key, 0.0, 1.0)\n        print(console.render(v))\n        ```\n        Note that you often do want or need to invoke `sample` directly - you'll likely want to use the generative function interface methods instead:\n        ```python exec=\"yes\" source=\"tabbed-left\"\n        import jax\n        import genjax\n        console = genjax.pretty()\n        key = jax.random.PRNGKey(314159)\n        key, tr = genjax.normal.simulate(key, (0.0, 1.0))\n        print(console.render(tr))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/index.html#genjax._src.generative_functions.distributions.distribution.ExactDensity.logpdf","title":"<code>logpdf(v, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a value from the event space, compute the log probability of that value under the distribution.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abc.abstractmethod\ndef logpdf(self, v, *args, **kwargs):\n\"\"\"&gt; Given a value from the event space, compute the log probability of\n    that value under the distribution.\"\"\"\n</code></pre>"},{"location":"genjax/library/generative_functions/distributions/scipy.html","title":"SciPy","text":"<p>Here, we document the GenJAX distributions which implement the generative function interfaces using <code>scipy</code> distribution methods. </p> <p>Each shorthand name (<code>module-attribute</code> below) refers to an instance of an indicator class which implements the interfaces for <code>ExactDensity</code> using the <code>scipy</code> distribution methods. </p> <p>These indicator classes are instanced when <code>genjax</code> is loaded, for convenience (so that users need not instance their own <code>Beta()</code>, etc).</p>"},{"location":"genjax/library/generative_functions/distributions/scipy.html#beta","title":"Beta","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.beta","title":"<code>genjax.generative_functions.distributions.beta = Beta()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#bernoulli","title":"Bernoulli","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.bernoulli","title":"<code>genjax.generative_functions.distributions.bernoulli = Bernoulli()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#cauchy","title":"Cauchy","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.cauchy","title":"<code>genjax.generative_functions.distributions.cauchy = Cauchy()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#categorical","title":"Categorical","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.categorical","title":"<code>genjax.generative_functions.distributions.categorical = Categorical()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#dirichlet","title":"Dirichlet","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.dirichlet","title":"<code>genjax.generative_functions.distributions.dirichlet = Dirichlet()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#exponential","title":"Exponential","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.exponential","title":"<code>genjax.generative_functions.distributions.exponential = Exponential()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#gamma","title":"Gamma","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.gamma","title":"<code>genjax.generative_functions.distributions.gamma = Gamma()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#laplace","title":"Laplace","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.laplace","title":"<code>genjax.generative_functions.distributions.laplace = Laplace()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#logistic","title":"Logistic","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.logistic","title":"<code>genjax.generative_functions.distributions.logistic = Logistic()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#multivariate-normal","title":"Multivariate normal","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.mv_normal","title":"<code>genjax.generative_functions.distributions.mv_normal = MultivariateNormal()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#normal","title":"Normal","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.normal","title":"<code>genjax.generative_functions.distributions.normal = Normal()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#pareto","title":"Pareto","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.pareto","title":"<code>genjax.generative_functions.distributions.pareto = Pareto()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#poisson","title":"Poisson","text":""},{"location":"genjax/library/generative_functions/distributions/scipy.html#genjax.generative_functions.distributions.poisson","title":"<code>genjax.generative_functions.distributions.poisson = Poisson()</code>  <code>module-attribute</code>","text":""},{"location":"genjax/library/inference/index.html","title":"Inference","text":"<p><code>genjax</code> exposes several inference algorithms which are implemented utilizing the generative function interface.</p>"},{"location":"genjax/library/inference/is.html","title":"Importance sampling","text":"<p>This module exposes two variants of importance sampling, differing in their return signature.</p> <p>The first is <code>ImportanceSampling</code>.</p> <p>Sampling importance resampling runs importance sampling, and then resamples a single particle from the particle collection to return.</p>"},{"location":"genjax/library/inference/is.html#genjax.inference.ImportanceSampling","title":"<code>genjax.inference.ImportanceSampling</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Bootstrap and proposal importance sampling for generative functions.</p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass ImportanceSampling(Pytree):\n\"\"\"Bootstrap and proposal importance sampling for generative functions.\"\"\"\nnum_particles: IntArray\nmodel: GenerativeFunction\nproposal: Union[None, GenerativeFunction] = None\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@typecheck\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn ImportanceSampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\ndef _bootstrap_importance_sampling(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\n):\nkey, sub_keys = slash(key, self.num_particles)\n_, (lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nmodel_args,\n)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (trs, log_normalized_weights, log_ml_estimate)\ndef _proposal_importance_sampling(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\nproposal_args: Tuple,\n):\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n_, p_trs = jax.vmap(self.proposal.simulate, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nproposal_args,\n)\nobservations = jax.tree_util.map(\nlambda v: jnp.repeats(v, self.num_particles), observations\n)\nchm = p_trs.get_choices().merge(observations)\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n_, (lws, m_trs) = jax.vmap(self.model.importance, in_axes=(0, 0, None))(\nsub_keys,\nchm,\nmodel_args,\n)\nlws = lws - p_trs.get_score()\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nreturn key, (m_trs, log_normalized_weights, log_ml_estimate)\n@typecheck\ndef apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance sampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_sampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance sampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_sampling(key, choice_map, model_args)\n@typecheck\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.ImportanceSampling.new","title":"<code>new(num_particles, model, proposal=None)</code>  <code>classmethod</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@typecheck\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn ImportanceSampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.ImportanceSampling.apply","title":"<code>apply(key, choice_map, *args)</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@typecheck\ndef apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance sampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_sampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance sampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_sampling(key, choice_map, model_args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax.inference.SamplingImportanceResampling","title":"<code>genjax.inference.SamplingImportanceResampling</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@dataclasses.dataclass\nclass SamplingImportanceResampling(Pytree):\nnum_particles: IntArray\nmodel: GenerativeFunction\nproposal: Union[None, GenerativeFunction] = None\ndef flatten(self):\nreturn (), (self.num_particles, self.model, self.proposal)\n@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn SamplingImportanceResampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\ndef _bootstrap_importance_resampling(\nself,\nkey: PRNGKey,\nobs: ChoiceMap,\nmodel_args: Tuple,\n):\nkey, sub_keys = slash(key, self.num_particles)\n_, (lws, trs) = jax.vmap(self.model.importance, in_axes=(0, None, None))(\nsub_keys, obs, model_args\n)\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nkey, sub_key = jax.random.split(key)\nind = jax.random.categorical(sub_key, log_normalized_weights)\ntr = jax.tree_util.tree_map(lambda v: v[ind], trs)\nlnw = log_normalized_weights[ind]\nreturn key, (tr, lnw, log_ml_estimate)\ndef _proposal_importance_resampling(\nself,\nkey: PRNGKey,\nobservations: ChoiceMap,\nmodel_args: Tuple,\nproposal_args: Tuple,\n):\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n_, p_trs = jax.vmap(self.proposal.simulate, in_axes=(0, None, None))(\nsub_keys,\nobservations,\nproposal_args,\n)\nobservations = jax.tree_util.map(\nlambda v: jnp.repeats(v, self.num_particles), observations\n)\nchm = p_trs.get_choices().merge(observations)\nkey, *sub_keys = jax.random.split(key, self.num_particles + 1)\nsub_keys = jnp.array(sub_keys)\n_, (lws, m_trs) = jax.vmap(self.model.importance, in_axes=(0, 0, None))(\nsub_keys,\nchm,\nmodel_args,\n)\nlws = lws - p_trs.get_score()\nlog_total_weight = jax.scipy.special.logsumexp(lws)\nlog_normalized_weights = lws - log_total_weight\nlog_ml_estimate = log_total_weight - jnp.log(self.num_particles)\nkey, sub_key = jax.random.split(key)\nind = jax.random.categorical(sub_key, log_normalized_weights)\ntr = jax.tree_util.tree_map(lambda v: v[ind], p_trs)\nlnw = log_normalized_weights[ind]\nreturn key, (tr, lnw, log_ml_estimate)\ndef apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance resampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_resampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance resampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_resampling(key, choice_map, model_args)\ndef __call__(self, key: PRNGKey, choice_map: ChoiceMap, *args):\nreturn self.apply(key, choice_map, *args)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.SamplingImportanceResampling.new","title":"<code>new(num_particles, model, proposal=None)</code>  <code>classmethod</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>@classmethod\ndef new(\ncls,\nnum_particles: IntArray,\nmodel: GenerativeFunction,\nproposal: Union[None, GenerativeFunction] = None,\n):\nreturn SamplingImportanceResampling(\nnum_particles,\nmodel,\nproposal=proposal,\n)\n</code></pre>"},{"location":"genjax/library/inference/is.html#genjax._src.inference.importance_sampling.SamplingImportanceResampling.apply","title":"<code>apply(key, choice_map, *args)</code>","text":"Source code in <code>src/genjax/_src/inference/importance_sampling.py</code> <pre><code>def apply(self, key: PRNGKey, choice_map: ChoiceMap, *args):\n# Importance resampling with custom proposal branch.\nif len(args) == 2:\nassert isinstance(args[0], tuple)\nassert isinstance(args[1], tuple)\nassert self.proposal is not None\nmodel_args = args[0]\nproposal_args = args[1]\nreturn self._proposal_importance_resampling(\nkey, choice_map, model_args, proposal_args\n)\n# Bootstrap importance resampling branch.\nelse:\nassert isinstance(args, tuple)\nassert self.proposal is None\nmodel_args = args[0]\nreturn self._bootstrap_importance_resampling(key, choice_map, model_args)\n</code></pre>"}]}